{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1_Pre_processing.ipynb","provenance":[],"collapsed_sections":["rNU64Cr32yYx","Q8PxlZBH3lvg","T8pZAGfw4PoA","LiJQHNpJ4jGW","eyJd7M7g42nn","lAQpwNoW2lGW","xmrFiQGNDTgL","sPx-5CjEEhZT","R1cijGyfHdJc","5dtdaIYgJZH9","haNDIyCFyAEE","R5xz9uyAz2ND","1TJSDEIw1A35","J9p38sgG6DIG","0EAWrY-W5_ju","dzlq2jVV8VC9","uc6-1SmrEEON","u4XEY5ps__VU","4elbD45oDXmy"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"4-xYjXquUlCo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628534081859,"user_tz":240,"elapsed":13875,"user":{"displayName":"Nora Williams","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi0jZN7Qe3fX_kgPpoUwG9G_NV9QayR0wwC8kZOLg=s64","userId":"08780686863156083883"}},"outputId":"ffa2cd6d-4a0e-4af1-8288-3ff599cb60d2"},"source":["!pip install nltk\n","!pip install numpy matplotlib\n","!pip install pandas\n","!pip install wordcloud"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n","Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.19.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1hL54Tlp88wf"},"source":["# download relevant parts of NLTK\n","import nltk\n","nltk.download('all')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-hlZwDHZkThA"},"source":["# Overview\n","\n","## 1. Regular Expression\n","\n","## 2. Basics about NLTK\n","1. Tokenization\n","\n","  1.   Sentence tokenization\n","  2.   Word tokenization\n","\n","2. Filtering Stop Words  \n","3.   Stemming\n","4.   Lemmatizing\n","\n","\n","## 3. Example of Analyzing Text\n","\n","## 4. Practice\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rNU64Cr32yYx"},"source":["# Regular Expression in Python\n","\n","In this section, we will briefly go through the basic regular experssions. For more details, please refer to https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial"]},{"cell_type":"code","metadata":{"id":"wtR8Ruru3EQ5"},"source":["import re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8PxlZBH3lvg"},"source":["## Disjunctions"]},{"cell_type":"code","metadata":{"id":"VzKQiWTL3F-J"},"source":["pattern = '[wW]oodchuck'\n","sequence = ['Woodchuck', 'woodchuck', 'wOodchuck']\n","\n","for word in sequence:\n","  print(word)\n","  if re.match(pattern, word):\n","    print('Match!')\n","  else:\n","    print('Not Match!')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hDUVRGzB3_tO"},"source":["pattern = '[0-9][a-z][A-Z]'\n","sequence = ['1sP', 'ssP', '7CS']\n","\n","for word in sequence:\n","  print(word)\n","  if re.match(pattern, word):\n","    print('Match!')\n","  else:\n","    print('Not Match!')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T8pZAGfw4PoA"},"source":["## Negation in Disjunction"]},{"cell_type":"code","metadata":{"id":"Wm7T8RhM4SM-"},"source":["pattern = '[0-9][^a-z][A-Z]'\n","sequence = ['1sP', 'ssP', '7CS']\n","\n","for word in sequence:\n","  print(word)\n","  if re.match(pattern, word):\n","    print('Match!')\n","  else:\n","    print('Not Match!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LiJQHNpJ4jGW"},"source":["## More Disjunction"]},{"cell_type":"code","metadata":{"id":"tBNxyaF94she"},"source":["pattern = '[gG]roundhog|[Ww]oodchuck'\n","sequence = ['groundhog', 'woodchuck', 'wOOdchuck']\n","\n","for word in sequence:\n","  print(word)\n","  if re.match(pattern, word):\n","    print('Match!')\n","  else:\n","    print('Not Match!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eyJd7M7g42nn"},"source":["## Regular Experssion with ```? *+.$```\n","*italicized text*\n"]},{"cell_type":"markdown","metadata":{"id":"644NM8aG5MuA"},"source":["`.` - A period. Matches any single character except the newline character."]},{"cell_type":"code","metadata":{"id":"HILEGlsP5jCW"},"source":["# With the search function, you scan through the given string/sequence, looking for the first location where the regular expression produces a match.\n","# The group function returns the string matched by the re.\n","\n","re.search(r'Co.k.e', 'Co\\noie Cookie cookie').group()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"87-ulJvD6LO9"},"source":["`^` - A caret. Matches the start of the string."]},{"cell_type":"code","metadata":{"id":"HxyM9Ejo58hp"},"source":["re.search(r'^Eat', \"Eat cake!\").group()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"StIpfn_Y6S9C"},"source":["re.search(r'^eat', \"Let's eat cake!\").group()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lBZSveAW6X1a"},"source":["$ - Matches the end of string."]},{"cell_type":"code","metadata":{"id":"dnMzYlIK6eyM"},"source":["re.search(r'cake$', \"Cake! Let's eat cake\").group()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h3VtC-2f6hih"},"source":["re.search(r'cake$', \"Let's get some cake on our way home!\").group()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VjojmY6a6j3G"},"source":["`+` - Checks if the preceding character appears one or more times starting from that position."]},{"cell_type":"code","metadata":{"id":"DmA-M7oU6pXw"},"source":["re.search(r'Co+kie', 'Cooookie').group()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l3KwNGXq6sPf"},"source":["`*` - Checks if the preceding character appears zero or more times starting from that position."]},{"cell_type":"code","metadata":{"id":"QacJ5chZ6vkV"},"source":["re.search(r'Ca*o*kie', 'Cookie').group()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zj30KCyp61Ko"},"source":["? - Checks if the preceding character appears exactly zero or one time starting from that position."]},{"cell_type":"code","metadata":{"id":"sUHQkplX65Y8"},"source":["re.search(r'Colou?r', 'Color').group()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P57kiIaK6_5B"},"source":["re.search(r'Colou?r', 'Colour').group()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yivOxJbL69C_"},"source":["re.search(r'Colou?r', 'Colouur').group()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAQpwNoW2lGW"},"source":["# Basics about NLTK"]},{"cell_type":"markdown","metadata":{"id":"xmrFiQGNDTgL"},"source":["## Tokenization\n","\n","Tokenization is the first step in turning unstructured data into structured data, which is easier to analyze.\n","\n","Through tokenization, you could split up text by word or by sentence, which could allow you to work with smaller pieces of text.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sPx-5CjEEhZT"},"source":["### Sentence tokenization\n"]},{"cell_type":"code","metadata":{"id":"ld05A2aQDsA-"},"source":["# import relevant parts of NLTK\n","\n","from nltk.tokenize import sent_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e9vqO16TFMAw"},"source":["example_string = \"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b0-ViwgyGFk_"},"source":["# Tokenizing example_string by sentence gives you a list of three strings that are sentences:\n","sentences = sent_tokenize(example_string)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ZnBfK7-H1Wl"},"source":["sentences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R1cijGyfHdJc"},"source":["### Word tokenization"]},{"cell_type":"code","metadata":{"id":"sMVLDBB2GHWx"},"source":["from nltk.tokenize import word_tokenize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d_s8y5CUHiBx"},"source":["words = word_tokenize(sentences[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rICATuTwJJho"},"source":["words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5jIwGOmLJKLu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5dtdaIYgJZH9"},"source":["## Filtering Stop Words \n","\n","Stop words are a set of commonly used words in a language. Very common words like 'in', 'is', and 'an' are often used as stop words since they don’t add a lot of meaning and information to a text in and of themselves. In many cases like topic extraction, you would like to ignore and filter them out of your text when you’re processing it."]},{"cell_type":"code","metadata":{"id":"pCVOKwMWLmsc"},"source":["from nltk.corpus import stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBotCuZCLsML"},"source":["stop_words = set(stopwords.words(\"english\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xT332iIBMGVF"},"source":["stop_words\n","# You can always design your own stop_words list depending on the task\n","# for example, you might want to filter out punctuations\n","# import string\n","# string.punctuation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lNeNFLG2xqQ6"},"source":["words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eqCxWWmcLvMP"},"source":["filtered_list = []\n","\n","for word in words:\n","  if word.casefold() not in stop_words:\n","    filtered_list.append(word.lower())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VDEtvbqeL8t0"},"source":["filtered_list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"haNDIyCFyAEE"},"source":["## Stemming\n","\n","Stemming is a text processing task in which you reduce words to their root, which is the core part of a word. For example, the words “helping” and “helper” share the root “help.” Stemming allows you to zero in on the basic meaning of a word rather than all the details of how it’s being used. "]},{"cell_type":"code","metadata":{"id":"HBjCtWjbL9fK"},"source":["from nltk.stem import SnowballStemmer\n","stemmer = SnowballStemmer('english')\n","\n","# there are also other types of stenmmer in NLTK such as porter stemmer\n","# from nltk.stem import PorterStemmer\n","# stemmer = PorterStemmer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zNXBHH7PySSm"},"source":["string_for_stemming = \"\"\"The crew of the USS Discovery discovered many discoveries. Discovering is what explorers do.\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKDq0QFTyfhu"},"source":["words = word_tokenize(string_for_stemming)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U8gF6ZC2ygm5"},"source":["words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZDr_OsXxyh9A"},"source":["stemmed_words = []\n","\n","for word in words:\n","  stemmed_words.append(stemmer.stem(word))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nb7vf_TypRw"},"source":["stemmed_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R5xz9uyAz2ND"},"source":["## Lemmatizing\n","\n","Like stemming, lemmatizing reduces words to their core meaning, but it will give you a complete English word that makes sense on its own instead of just a fragment of a word like 'discoveri'."]},{"cell_type":"code","metadata":{"id":"2Vc9Nqhe0JTZ"},"source":["from nltk.stem import WordNetLemmatizer\n","\n","lemmatizer = WordNetLemmatizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KmdResMk0MSq"},"source":["lemmatizer.lemmatize(\"discoveries\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HMQ-T3H20PNM"},"source":["string_for_lemmatizing = \"\"\"The students like NLP experiments.\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lbvbwTgY0Ty_"},"source":["words = word_tokenize(string_for_lemmatizing)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S2PTyiaN0czI"},"source":["words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"intNoN6W0UPN"},"source":["lemmatized_words = []\n","for word in words:\n","  lemmatized_words.append(lemmatizer.lemmatize(word))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RAWMEXHW0bib"},"source":["lemmatized_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D2933NvR0cVC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1TJSDEIw1A35"},"source":["# Example of Analyzing Text\n","\n","1. Building the word dictionary, \n","2. Visualizing the lengths of newsgroups messages,\n","3. Visualizing the word frequency distribution.\n","\n"]},{"cell_type":"code","metadata":{"id":"GCBErJ8A1DVp"},"source":["# Importing all the related packages\n","import pandas as pd\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import string\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from wordcloud import WordCloud"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qAWpN2Pe1qFR"},"source":["# Here we use 20-Newsgroups dataset (http://qwone.com/~jason/20Newsgroups/) for this example. \n","# This version of the dataset contains about 11k newsgroups posts from 20 different topics. \n","# This is available as https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json\n","\n","raw_data = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n","print(raw_data.target_names.unique())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aoA4av_62PmU"},"source":["raw_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XDiqRVj45bxH"},"source":["# let's use tweets that are classified as rec.sport.baseball as an example\n","text = []\n","for i in range(0, len(raw_data['content'])):\n","  if raw_data['target_names'][i] == 'rec.sport.baseball':\n","    text.append(raw_data['content'][i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WK5SHjWs5uIy"},"source":["len(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtmk5xsGLZCW"},"source":["text[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J9p38sgG6DIG"},"source":["## Preprocessing and Tokenization"]},{"cell_type":"code","metadata":{"id":"2EsHb9JY5wiD"},"source":["# Tokenizing\n","tokenized_text = []\n","for sentence in text:\n","  tokenized_text.append(word_tokenize(sentence))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h5vd2vyO6TCk"},"source":["tokenized_text[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0EAWrY-W5_ju"},"source":["## Filtering Stop Words and Punctuations, and Lemmatizing"]},{"cell_type":"code","metadata":{"id":"HVE-OPPr7iAS"},"source":["# Define the stop word set\n","stop_words = stopwords.words(\"english\")\n","stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'line', \"'s\", \"n't\", \"'d\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4H024R-xCU_F"},"source":["# Define the punctuation set\n","punctuations = string.punctuation  + \"*\" + \"/\" + \"\\\\\" + \"_\" + \"-\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYCGP9DLQe-U"},"source":["# Define the lemmatizer\n","lemmatizer = WordNetLemmatizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pdhec8Ur743u"},"source":["filtered_text = []\n","\n","for sent in tokenized_text:\n","  filtered_list = []\n","  for word in sent:\n","    # filter out tokens that have punctuations and numbers\n","      # word.isalpha() returns true if a string only contains letters.\n","    # filter out stop words\n","    if word.isalpha() and lemmatizer.lemmatize(word.lower()) not in stop_words:\n","      filtered_list.append(lemmatizer.lemmatize(word.lower()))\n","  filtered_text.append(filtered_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOjnirnX8J0t"},"source":["filtered_text[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dzlq2jVV8VC9"},"source":["## Building Word Dictionary\n","\n","This step will build the word dictionary and then you could use the dictionary to transform text into ids or one-hot vectors for future tasks like classification"]},{"cell_type":"code","metadata":{"id":"ESFmVeoF8Zcu"},"source":["# Build the word dictionary\n","word2id = {}\n","id2word = {}\n","\n","word_id = 0\n","for sent in filtered_text:\n","  for word in sent:\n","    if word not in word2id:\n","      word2id[word] = word_id\n","      id2word[word_id] = word\n","      word_id += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PV3Xu5KY8gMl"},"source":["# the number of different word\n","len(word2id)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"StSpkZ-x_FJR"},"source":["word2id['tell']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_DiDVjXB_LqF"},"source":["# representing text in ids\n","word_ids = []\n","for word in filtered_text[10]:\n","  word_ids.append(word2id[word])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IF7chAxn_Uyx"},"source":["word_ids"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8Lbkaa__jFA"},"source":["# translating ids back to text\n","sentence = []\n","for word_id in word_ids:\n","  sentence.append(id2word[word_id])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qX3POBC_wDb"},"source":["sentence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uc6-1SmrEEON"},"source":["## Visualzing News Lengths"]},{"cell_type":"code","metadata":{"id":"DrQ70qMi_wkQ"},"source":["news_length = []\n","\n","for sent in filtered_text:\n","  news_length.append(len(sent))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"opxyaLhkFI10"},"source":["# Average lenths\n","sum(news_length)/len(news_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-7KxOrNVFbL6"},"source":["# visualizing the lengths distribution\n","\n","plt.hist(news_length)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u4XEY5ps__VU"},"source":["## Visualizing the Word Frequency"]},{"cell_type":"code","metadata":{"id":"UogutfoUACHJ"},"source":["from collections import Counter\n","\n","words = []\n","for sent in filtered_text:\n","  for word in sent:\n","    words.append(word)\n","\n","word_counts = Counter(words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Do2tQ4hxBiqA"},"source":["# checking the most common words in hate speech tweets\n","# You could find thet most common words in hate speech tweets are some racist, sexist, homophobic, and offensive words\n","word_counts.most_common(20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rsi5kyHGGTCa"},"source":["# Checking the least common words in hate speech tweets\n","word_counts.most_common(len(word_counts))[-20:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gdp8j4QtPBVz"},"source":["# Plot the word cloud image\n","from wordcloud import WordCloud\n","wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n","wordcloud.generate(','.join(words))\n","wordcloud.to_image()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4elbD45oDXmy"},"source":["# Practice\n","\n","Let's take a look at news that belongs to other categories (e.g., sci.med).\n","\n","1. What are the numbers of distinct words in messages that belongs to other categories (e.g., sci.med)?\n","2. What are the average lengths of those messages? Are the length distributions different from baseball messages?\n","3. What are the most common words and least common words in messages that belongs to other categories? Are they different from baseball messages?\n","\n"]},{"cell_type":"code","metadata":{"id":"B6eLr7sHG9cR"},"source":["med_text = []\n","for i in range(0, len(raw_data['content'])):\n","  if raw_data['target_names'][i] == 'sci.med':\n","    med_text.append(raw_data['content'][i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oXHHJx0cHNTH"},"source":["len(med_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHEk0JEpHKdG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3qk97zgMHMzW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QHXit1y9HPcM"},"source":[""],"execution_count":null,"outputs":[]}]}