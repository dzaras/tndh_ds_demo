{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Topic Modeling, Word Embeddings and Clustering.ipynb","provenance":[],"authorship_tag":"ABX9TyMz+9eitsGcSSLsBNseUssB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3P6G6g_iIduV","executionInfo":{"status":"ok","timestamp":1628452112766,"user_tz":240,"elapsed":14062,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"dda9b0e2-b9b8-4921-8ae4-6ec54f64b7c8"},"source":["!pip install nltk\n","!pip install numpy matplotlib\n","!pip install pandas\n","!pip install gensim\n","!pip install sklearn"],"execution_count":80,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uvBc305rIyv3","executionInfo":{"status":"ok","timestamp":1628393613254,"user_tz":240,"elapsed":13,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":[""],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fbVMgSbBKgEV"},"source":["# Overview\n","## 1. Topic Modeling with LDA\n","\n","## 2. Word Embeddings\n","\n","## 3. Sentence/Document Clustering Using Word Embeddings\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7v6SXSmLIJV","executionInfo":{"status":"ok","timestamp":1628449553604,"user_tz":240,"elapsed":2316,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"40ac0e8a-2835-442f-e59a-763b19d0e71a"},"source":["# download relevant parts of NLTK\n","import nltk\n","nltk.download('all')"],"execution_count":25,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Package abc is already up-to-date!\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Package alpino is already up-to-date!\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Package brown is already up-to-date!\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Package brown_tei is already up-to-date!\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Package cess_cat is already up-to-date!\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Package cess_esp is already up-to-date!\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Package chat80 is already up-to-date!\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package city_database is already up-to-date!\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package comparative_sentences is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    |   Package comtrans is already up-to-date!\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Package conll2000 is already up-to-date!\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Package conll2002 is already up-to-date!\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    |   Package conll2007 is already up-to-date!\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Package crubadan is already up-to-date!\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package dependency_treebank is already up-to-date!\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Package dolch is already up-to-date!\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package europarl_raw is already up-to-date!\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Package floresta is already up-to-date!\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package framenet_v15 is already up-to-date!\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package framenet_v17 is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Package ieer is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Package indian is already up-to-date!\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    |   Package jeita is already up-to-date!\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Package kimmo is already up-to-date!\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    |   Package knbc is already up-to-date!\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Package mac_morpho is already up-to-date!\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    |   Package machado is already up-to-date!\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    |   Package masc_tagged is already up-to-date!\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package moses_sample is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Package nps_chat is already up-to-date!\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Package paradigms is already up-to-date!\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Package pil is already up-to-date!\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Package pl196x is already up-to-date!\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Package ppattach is already up-to-date!\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package problem_reports is already up-to-date!\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    |   Package propbank is already up-to-date!\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Package ptb is already up-to-date!\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Package pros_cons is already up-to-date!\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Package qc is already up-to-date!\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    |   Package reuters is already up-to-date!\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Package rte is already up-to-date!\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    |   Package semcor is already up-to-date!\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Package senseval is already up-to-date!\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sentiwordnet is already up-to-date!\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sentence_polarity is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sinica_treebank is already up-to-date!\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Package smultron is already up-to-date!\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Package state_union is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package subjectivity is already up-to-date!\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Package swadesh is already up-to-date!\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Package switchboard is already up-to-date!\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Package timit is already up-to-date!\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Package toolbox is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Package udhr is already up-to-date!\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Package udhr2 is already up-to-date!\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package unicode_samples is already up-to-date!\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Package verbnet is already up-to-date!\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Package verbnet3 is already up-to-date!\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Package webtext is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Package ycoe is already up-to-date!\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Package rslp is already up-to-date!\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_tagset is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package book_grammars is already up-to-date!\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sample_grammars is already up-to-date!\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package spanish_grammars is already up-to-date!\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package basque_grammars is already up-to-date!\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package large_grammars is already up-to-date!\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Package tagsets is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package word2vec_sample is already up-to-date!\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Package mte_teip5 is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n","[nltk_data]    |       up-to-date!\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package perluniprops is already up-to-date!\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package vader_lexicon is already up-to-date!\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Package porter_test is already up-to-date!\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Package wmt15_eval is already up-to-date!\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WmU93h9ALxqO","executionInfo":{"status":"ok","timestamp":1628449554924,"user_tz":240,"elapsed":1322,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"261e2387-e3de-4672-932a-7e9ea8d72f18"},"source":["import pandas as pd\n","\n","# Here we use 20-Newsgroups dataset (http://qwone.com/~jason/20Newsgroups/) for this example. \n","# This version of the dataset contains about 11k newsgroups posts from 20 different topics. \n","# This is available as https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json\n","\n","raw_data = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n","print(raw_data.target_names.unique())"],"execution_count":26,"outputs":[{"output_type":"stream","text":["['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n"," 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n"," 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n"," 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n"," 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n"," 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"OSUXiWXCOe3X","executionInfo":{"status":"ok","timestamp":1628449554925,"user_tz":240,"elapsed":8,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"d655e2d4-1a3c-490d-c56a-bcb80fa15d32"},"source":["raw_data"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content</th>\n","      <th>target</th>\n","      <th>target_names</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n","      <td>7</td>\n","      <td>rec.autos</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n","      <td>4</td>\n","      <td>comp.sys.mac.hardware</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n","      <td>4</td>\n","      <td>comp.sys.mac.hardware</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n","      <td>1</td>\n","      <td>comp.graphics</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n","      <td>14</td>\n","      <td>sci.space</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11309</th>\n","      <td>From: jim.zisfein@factory.com (Jim Zisfein) \\n...</td>\n","      <td>13</td>\n","      <td>sci.med</td>\n","    </tr>\n","    <tr>\n","      <th>11310</th>\n","      <td>From: ebodin@pearl.tufts.edu\\nSubject: Screen ...</td>\n","      <td>4</td>\n","      <td>comp.sys.mac.hardware</td>\n","    </tr>\n","    <tr>\n","      <th>11311</th>\n","      <td>From: westes@netcom.com (Will Estes)\\nSubject:...</td>\n","      <td>3</td>\n","      <td>comp.sys.ibm.pc.hardware</td>\n","    </tr>\n","    <tr>\n","      <th>11312</th>\n","      <td>From: steve@hcrlgw (Steven Collins)\\nSubject: ...</td>\n","      <td>1</td>\n","      <td>comp.graphics</td>\n","    </tr>\n","    <tr>\n","      <th>11313</th>\n","      <td>From: gunning@cco.caltech.edu (Kevin J. Gunnin...</td>\n","      <td>8</td>\n","      <td>rec.motorcycles</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11314 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                 content  ...              target_names\n","0      From: lerxst@wam.umd.edu (where's my thing)\\nS...  ...                 rec.autos\n","1      From: guykuo@carson.u.washington.edu (Guy Kuo)...  ...     comp.sys.mac.hardware\n","2      From: twillis@ec.ecn.purdue.edu (Thomas E Will...  ...     comp.sys.mac.hardware\n","3      From: jgreen@amber (Joe Green)\\nSubject: Re: W...  ...             comp.graphics\n","4      From: jcm@head-cfa.harvard.edu (Jonathan McDow...  ...                 sci.space\n","...                                                  ...  ...                       ...\n","11309  From: jim.zisfein@factory.com (Jim Zisfein) \\n...  ...                   sci.med\n","11310  From: ebodin@pearl.tufts.edu\\nSubject: Screen ...  ...     comp.sys.mac.hardware\n","11311  From: westes@netcom.com (Will Estes)\\nSubject:...  ...  comp.sys.ibm.pc.hardware\n","11312  From: steve@hcrlgw (Steven Collins)\\nSubject: ...  ...             comp.graphics\n","11313  From: gunning@cco.caltech.edu (Kevin J. Gunnin...  ...           rec.motorcycles\n","\n","[11314 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"mmjBNkFRUW4k","executionInfo":{"status":"ok","timestamp":1628449554925,"user_tz":240,"elapsed":6,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["text = []\n","for i in range(0, len(raw_data['content'])):\n","  text.append(raw_data['content'][i])"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_FNMIpoTOn3S"},"source":["# Topic Modeling with LDA\n","\n","In this section, we will go through how to use python pacakages (*gensim*) to perform the topic analysis.\n","\n","Reference: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n"]},{"cell_type":"markdown","metadata":{"id":"0qamE_j5SBlg"},"source":["## Prepareing data for LDA Analysis\n","\n","As the first step, we will follow pre-processing methods leanred previously to pre-process the data like tokenization, filtering out the stop words, lemmatizing, building the word dictionary and etc."]},{"cell_type":"code","metadata":{"id":"DJAMkGD8Xfb5","executionInfo":{"status":"ok","timestamp":1628393678114,"user_tz":240,"elapsed":3,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["# Importing the needed packages\n","from nltk.tokenize import word_tokenize\n","\n","from nltk.corpus import stopwords\n","import string\n","from nltk.stem import WordNetLemmatizer\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"_0xUNRFNSE0p","executionInfo":{"status":"ok","timestamp":1628393709953,"user_tz":240,"elapsed":31842,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["# tokenization\n","tokenized_text = []\n","for sentence in text:\n","  tokenized_text.append(word_tokenize(sentence))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"EpF2-1_IUbse","executionInfo":{"status":"ok","timestamp":1628393739585,"user_tz":240,"elapsed":29644,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["# filtering stop words (numbers) and punctuations, and lemmatzing\n","stop_words = stopwords.words(\"english\")\n","stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'line', 'organization', 'university', 'wa', 'ha', \"'s\", \"n't\", \"'d\"])\n","\n","punctuations = string.punctuation  + \"*\" + \"/\" + \"\\\\\" + \"_\" + \"-\"\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","filtered_text = []\n","\n","for sent in tokenized_text:\n","  filtered_list = []\n","  for word in sent:\n","    # filter out tokens that have punctuations and numbers\n","      # word.isalpha() returns true if a string only contains letters.\n","    # filter out stop words\n","    if word.isalpha() and lemmatizer.lemmatize(word.lower()) not in stop_words and len(word) >= 2:\n","      filtered_list.append(lemmatizer.lemmatize(word.lower()))\n","  filtered_text.append(filtered_list)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7To7pe6vUx_1"},"source":["## Creating the Dictionary and Corpus needed for LDA Topic Modeling"]},{"cell_type":"code","metadata":{"id":"F8fwAC-hU4C7","executionInfo":{"status":"ok","timestamp":1628393741871,"user_tz":240,"elapsed":2297,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["import gensim.corpora as corpora\n","\n","# Create Dictionary\n","id2word = corpora.Dictionary(filtered_text)\n","\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Pqrr19PkVM_a","executionInfo":{"status":"ok","timestamp":1628393741875,"user_tz":240,"elapsed":6,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"0e3ba0b0-1def-493b-f56c-cbfc2ed434d9"},"source":["id2word[0] "],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'addition'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"2qHfkhchVNfr","executionInfo":{"status":"ok","timestamp":1628393743291,"user_tz":240,"elapsed":1420,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["# Create Corpus\n","texts = filtered_text\n","\n","# Coverting Text to Bag of Words features\n","corpus = [id2word.doc2bow(text) for text in texts]"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8xD85cAlVGmy","executionInfo":{"status":"ok","timestamp":1628393743293,"user_tz":240,"elapsed":11,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"cd8e3bda-7d21-4295-998f-ce39e68adac5"},"source":["corpus[0]\n","# (word_id, word_count)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 1),\n"," (1, 2),\n"," (2, 1),\n"," (3, 1),\n"," (4, 1),\n"," (5, 1),\n"," (6, 1),\n"," (7, 5),\n"," (8, 1),\n"," (9, 1),\n"," (10, 1),\n"," (11, 1),\n"," (12, 1),\n"," (13, 1),\n"," (14, 1),\n"," (15, 1),\n"," (16, 1),\n"," (17, 1),\n"," (18, 1),\n"," (19, 1),\n"," (20, 1),\n"," (21, 1),\n"," (22, 2),\n"," (23, 1),\n"," (24, 1),\n"," (25, 1),\n"," (26, 1),\n"," (27, 1),\n"," (28, 1),\n"," (29, 1),\n"," (30, 1),\n"," (31, 1),\n"," (32, 1),\n"," (33, 1),\n"," (34, 1),\n"," (35, 1),\n"," (36, 1),\n"," (37, 1),\n"," (38, 1),\n"," (39, 1),\n"," (40, 1),\n"," (41, 1),\n"," (42, 1),\n"," (43, 1),\n"," (44, 1),\n"," (45, 1)]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BDJQG8K4VK8g","executionInfo":{"status":"ok","timestamp":1628393743293,"user_tz":240,"elapsed":9,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"29b781c3-7fdc-45a9-ac6e-2277498c1380"},"source":["print(id2word[0])\n","id2word[0] in filtered_text[0]"],"execution_count":13,"outputs":[{"output_type":"stream","text":["addition\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azI5HQkmVZyP","executionInfo":{"status":"ok","timestamp":1628393743294,"user_tz":240,"elapsed":8,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"cb199f29-7872-440a-8fa8-52a415728218"},"source":["print(id2word[60])\n","id2word[57] in filtered_text[0]"],"execution_count":14,"outputs":[{"output_type":"stream","text":["done\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nd5j_d17WDF2","executionInfo":{"status":"ok","timestamp":1628393743294,"user_tz":240,"elapsed":6,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"d0e4ce73-4bec-426b-e1e1-5da7f0a28f3b"},"source":["# Human readable format of corpus (term-frequency)\n","[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[('addition', 1),\n","  ('anyone', 2),\n","  ('body', 1),\n","  ('bricklin', 1),\n","  ('brought', 1),\n","  ('bumper', 1),\n","  ('called', 1),\n","  ('car', 5),\n","  ('college', 1),\n","  ('could', 1),\n","  ('day', 1),\n","  ('door', 1),\n","  ('early', 1),\n","  ('engine', 1),\n","  ('enlighten', 1),\n","  ('front', 1),\n","  ('funky', 1),\n","  ('history', 1),\n","  ('il', 1),\n","  ('info', 1),\n","  ('know', 1),\n","  ('late', 1),\n","  ('lerxst', 2),\n","  ('looked', 1),\n","  ('looking', 1),\n","  ('made', 1),\n","  ('maryland', 1),\n","  ('model', 1),\n","  ('name', 1),\n","  ('neighborhood', 1),\n","  ('park', 1),\n","  ('please', 1),\n","  ('production', 1),\n","  ('really', 1),\n","  ('rest', 1),\n","  ('saw', 1),\n","  ('separate', 1),\n","  ('small', 1),\n","  ('spec', 1),\n","  ('sport', 1),\n","  ('tellme', 1),\n","  ('thanks', 1),\n","  ('thing', 1),\n","  ('whatever', 1),\n","  ('wondering', 1),\n","  ('year', 1)]]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"kVnPb5rhWgrt"},"source":["## Building the LDA Model\n","\n","In this section we will learn the LDA model thourgh the LDA module in gensim. (https://radimrehurek.com/gensim/models/ldamodel.html)"]},{"cell_type":"code","metadata":{"id":"NZiGinh6WkIx","executionInfo":{"status":"ok","timestamp":1628393868768,"user_tz":240,"elapsed":125478,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["import gensim\n","lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                           id2word=id2word,\n","                                           num_topics=20, \n","                                           random_state= 0,\n","                                           passes = 10,\n","                                           alpha='auto')\n","\n","# for more details, please refers to: https://radimrehurek.com/gensim/models/ldamodel.html"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MPWjMc4pXsSe"},"source":["## View the topics in LDA Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8U--9QvBXvXN","executionInfo":{"status":"ok","timestamp":1628393868769,"user_tz":240,"elapsed":37,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"e7c4e454-0426-4cc7-970f-3975b24b3ec9"},"source":["lda_model.print_topics()"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0,\n","  '0.033*\"god\" + 0.014*\"christian\" + 0.013*\"jesus\" + 0.009*\"bible\" + 0.008*\"one\" + 0.007*\"church\" + 0.006*\"christ\" + 0.006*\"faith\" + 0.005*\"say\" + 0.005*\"religion\"'),\n"," (1,\n","  '0.016*\"space\" + 0.005*\"nasa\" + 0.005*\"would\" + 0.004*\"moon\" + 0.004*\"earth\" + 0.004*\"launch\" + 0.004*\"orbit\" + 0.004*\"year\" + 0.004*\"first\" + 0.004*\"henry\"'),\n"," (2,\n","  '0.015*\"detector\" + 0.012*\"radar\" + 0.006*\"value\" + 0.006*\"insurance\" + 0.005*\"brian\" + 0.005*\"health\" + 0.005*\"private\" + 0.005*\"widget\" + 0.004*\"set\" + 0.003*\"crohn\"'),\n"," (3,\n","  '0.011*\"gun\" + 0.011*\"law\" + 0.010*\"state\" + 0.010*\"right\" + 0.008*\"would\" + 0.008*\"people\" + 0.007*\"government\" + 0.006*\"article\" + 0.006*\"writes\" + 0.005*\"weapon\"'),\n"," (4,\n","  '0.015*\"writes\" + 0.012*\"article\" + 0.010*\"georgia\" + 0.008*\"helmet\" + 0.008*\"michael\" + 0.004*\"athens\" + 0.004*\"distribution\" + 0.004*\"like\" + 0.004*\"john\" + 0.004*\"jewish\"'),\n"," (5,\n","  '0.009*\"food\" + 0.008*\"israel\" + 0.008*\"disease\" + 0.008*\"msg\" + 0.007*\"medical\" + 0.006*\"article\" + 0.006*\"patient\" + 0.006*\"health\" + 0.006*\"writes\" + 0.005*\"child\"'),\n"," (6,\n","  '0.027*\"drive\" + 0.015*\"card\" + 0.011*\"disk\" + 0.009*\"system\" + 0.008*\"scsi\" + 0.008*\"sale\" + 0.008*\"hard\" + 0.008*\"driver\" + 0.007*\"controller\" + 0.007*\"mac\"'),\n"," (7,\n","  '0.021*\"book\" + 0.010*\"science\" + 0.006*\"bos\" + 0.006*\"copy\" + 0.006*\"new\" + 0.006*\"van\" + 0.005*\"det\" + 0.005*\"cover\" + 0.005*\"pit\" + 0.005*\"art\"'),\n"," (8,\n","  '0.022*\"key\" + 0.014*\"chip\" + 0.009*\"encryption\" + 0.009*\"clipper\" + 0.008*\"system\" + 0.007*\"would\" + 0.006*\"one\" + 0.006*\"data\" + 0.006*\"phone\" + 0.006*\"algorithm\"'),\n"," (9,\n","  '0.018*\"game\" + 0.017*\"team\" + 0.010*\"player\" + 0.010*\"year\" + 0.007*\"play\" + 0.007*\"hockey\" + 0.007*\"win\" + 0.006*\"season\" + 0.006*\"go\" + 0.005*\"last\"'),\n"," (10,\n","  '0.016*\"car\" + 0.009*\"would\" + 0.008*\"writes\" + 0.008*\"like\" + 0.008*\"article\" + 0.007*\"one\" + 0.007*\"get\" + 0.006*\"bike\" + 0.005*\"time\" + 0.005*\"good\"'),\n"," (11,\n","  '0.013*\"article\" + 0.012*\"writes\" + 0.010*\"bank\" + 0.009*\"gordon\" + 0.007*\"computer\" + 0.007*\"pittsburgh\" + 0.006*\"geb\" + 0.006*\"usa\" + 0.006*\"science\" + 0.005*\"pain\"'),\n"," (12,\n","  '0.140*\"max\" + 0.008*\"giz\" + 0.008*\"wm\" + 0.007*\"ah\" + 0.006*\"mv\" + 0.005*\"ql\" + 0.005*\"air\" + 0.005*\"mm\" + 0.004*\"mr\" + 0.004*\"fij\"'),\n"," (13,\n","  '0.013*\"entry\" + 0.013*\"file\" + 0.007*\"may\" + 0.007*\"program\" + 0.007*\"section\" + 0.005*\"number\" + 0.005*\"wire\" + 0.005*\"rule\" + 0.005*\"ground\" + 0.005*\"new\"'),\n"," (14,\n","  '0.009*\"people\" + 0.007*\"armenian\" + 0.007*\"said\" + 0.006*\"one\" + 0.005*\"u\" + 0.005*\"would\" + 0.005*\"jew\" + 0.005*\"know\" + 0.004*\"time\" + 0.004*\"turkish\"'),\n"," (15,\n","  '0.012*\"would\" + 0.012*\"one\" + 0.010*\"people\" + 0.010*\"think\" + 0.010*\"writes\" + 0.008*\"doe\" + 0.008*\"article\" + 0.007*\"like\" + 0.007*\"know\" + 0.007*\"say\"'),\n"," (16,\n","  '0.010*\"homosexual\" + 0.010*\"men\" + 0.008*\"gay\" + 0.008*\"cramer\" + 0.008*\"san\" + 0.007*\"sexual\" + 0.007*\"sex\" + 0.006*\"male\" + 0.006*\"new\" + 0.006*\"gld\"'),\n"," (17,\n","  '0.009*\"de\" + 0.007*\"card\" + 0.007*\"monitor\" + 0.007*\"key\" + 0.005*\"tower\" + 0.005*\"know\" + 0.005*\"water\" + 0.004*\"vga\" + 0.004*\"nubus\" + 0.004*\"mode\"'),\n"," (18,\n","  '0.015*\"window\" + 0.011*\"file\" + 0.007*\"program\" + 0.006*\"thanks\" + 0.006*\"version\" + 0.006*\"problem\" + 0.006*\"get\" + 0.006*\"system\" + 0.006*\"software\" + 0.005*\"doe\"'),\n"," (19,\n","  '0.009*\"tax\" + 0.008*\"new\" + 0.006*\"power\" + 0.005*\"insurance\" + 0.005*\"writes\" + 0.004*\"pay\" + 0.004*\"company\" + 0.004*\"money\" + 0.004*\"one\" + 0.004*\"sale\"')]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"5osdPwa7YEMd"},"source":["### How to interpret the extracted topics?\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"V6cV5c4NYpj4"},"source":["For example, Topic 0 is represented as \n","\n","```\n","0.033*\"god\" + 0.014*\"christian\" + 0.013*\"jesus\" + 0.009*\"bible\" + 0.008*\"one\" + 0.007*\"church\" + 0.006*\"christ\" + 0.006*\"faith\" + 0.005*\"say\" + 0.005*\"religion\"\n","```\n","\n","It means the top 10 keywords that contribute to this topic are: god, christian, jesus, bible and etc. And the weight of food on topic 0 is 0.033.\n","\n","Looking at these keywords, can you guess what this topic could be? You may summarise it either are 'religion' or 'christian.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"OOnLIkbsYBS7","executionInfo":{"status":"ok","timestamp":1628393868769,"user_tz":240,"elapsed":34,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["# Getting labels for all the news\n","doc_lda = lda_model[corpus]"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GE5KHoRJdf81","executionInfo":{"status":"ok","timestamp":1628393868770,"user_tz":240,"elapsed":34,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"fa1e6992-227d-430a-e48b-3e1778b4096d"},"source":["print(text[0])"],"execution_count":19,"outputs":[{"output_type":"stream","text":["From: lerxst@wam.umd.edu (where's my thing)\n","Subject: WHAT car is this!?\n","Nntp-Posting-Host: rac3.wam.umd.edu\n","Organization: University of Maryland, College Park\n","Lines: 15\n","\n"," I was wondering if anyone out there could enlighten me on this car I saw\n","the other day. It was a 2-door sports car, looked to be from the late 60s/\n","early 70s. It was called a Bricklin. The doors were really small. In addition,\n","the front bumper was separate from the rest of the body. This is \n","all I know. If anyone can tellme a model name, engine specs, years\n","of production, where this car is made, history, or whatever info you\n","have on this funky looking car, please e-mail.\n","\n","Thanks,\n","- IL\n","   ---- brought to you by your neighborhood Lerxst ----\n","\n","\n","\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3GhLnc0kRAD","executionInfo":{"status":"ok","timestamp":1628393868770,"user_tz":240,"elapsed":30,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"5f0ef5a9-a6e1-4d91-f14d-12d833b3f004"},"source":["doc_lda[0]"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(10, 0.6328957), (11, 0.3343146)]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jE1LQKQFkSwa","executionInfo":{"status":"ok","timestamp":1628393868771,"user_tz":240,"elapsed":28,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"df7f74f0-3197-4069-ffd0-58ef3f6191d7"},"source":["lda_model.print_topics()[10]"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10,\n"," '0.016*\"car\" + 0.009*\"would\" + 0.008*\"writes\" + 0.008*\"like\" + 0.008*\"article\" + 0.007*\"one\" + 0.007*\"get\" + 0.006*\"bike\" + 0.005*\"time\" + 0.005*\"good\"')"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WjThuCwQtH3U","executionInfo":{"status":"ok","timestamp":1628393868772,"user_tz":240,"elapsed":27,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"751fca9a-b5fe-414c-d444-bc32a183187e"},"source":["print(text[1])"],"execution_count":22,"outputs":[{"output_type":"stream","text":["From: guykuo@carson.u.washington.edu (Guy Kuo)\n","Subject: SI Clock Poll - Final Call\n","Summary: Final call for SI clock reports\n","Keywords: SI,acceleration,clock,upgrade\n","Article-I.D.: shelley.1qvfo9INNc3s\n","Organization: University of Washington\n","Lines: 11\n","NNTP-Posting-Host: carson.u.washington.edu\n","\n","A fair number of brave souls who upgraded their SI clock oscillator have\n","shared their experiences for this poll. Please send a brief message detailing\n","your experiences with the procedure. Top speed attained, CPU rated speed,\n","add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n","functionality with 800 and 1.4 m floppies are especially requested.\n","\n","I will be summarizing in the next two days, so please add to the network\n","knowledge base if you have done the clock upgrade and haven't answered this\n","poll. Thanks.\n","\n","Guy Kuo <guykuo@u.washington.edu>\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eSyc5nqhtLcE","executionInfo":{"status":"ok","timestamp":1628393868772,"user_tz":240,"elapsed":25,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"8539c4fb-a2dc-44a8-c51d-5b8436f44b3e"},"source":["doc_lda[1]"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, 0.10218506),\n"," (6, 0.387976),\n"," (9, 0.05716874),\n"," (13, 0.16183329),\n"," (15, 0.047197845),\n"," (18, 0.23120497)]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SU-fVmNFtM7V","executionInfo":{"status":"ok","timestamp":1628393868772,"user_tz":240,"elapsed":23,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"36ab1c40-ce5f-488e-ef7f-fb5a7078daaa"},"source":["lda_model.print_topics()[6]"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6,\n"," '0.027*\"drive\" + 0.015*\"card\" + 0.011*\"disk\" + 0.009*\"system\" + 0.008*\"scsi\" + 0.008*\"sale\" + 0.008*\"hard\" + 0.008*\"driver\" + 0.007*\"controller\" + 0.007*\"mac\"')"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"MgA3PBsebSSm"},"source":["### Practice: \n","What about the other topics? Can you try to summariza the topics based on these key words?"]},{"cell_type":"markdown","metadata":{"id":"yESfNr_EydvY"},"source":["# Word Embeddings\n","\n","In this section, we will learn how to use pre-trained embeddings from gensim to represent words with vectors."]},{"cell_type":"markdown","metadata":{"id":"hPkyePM2y1i-"},"source":["### Load Pre-trained Word Embeddings from Gensim"]},{"cell_type":"code","metadata":{"id":"KyhH5zXAyfyD","executionInfo":{"status":"ok","timestamp":1628449931555,"user_tz":240,"elapsed":247,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["import numpy as np\n","import gensim.downloader"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iS2cFnDcyZN4","executionInfo":{"status":"ok","timestamp":1628449555195,"user_tz":240,"elapsed":275,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"584748dc-683c-4c87-eb96-c35e0f8a6e09"},"source":["# View all types of pre-trained embeddings \n","list(gensim.downloader.info()['models'].keys())"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['fasttext-wiki-news-subwords-300',\n"," 'conceptnet-numberbatch-17-06-300',\n"," 'word2vec-ruscorpora-300',\n"," 'word2vec-google-news-300',\n"," 'glove-wiki-gigaword-50',\n"," 'glove-wiki-gigaword-100',\n"," 'glove-wiki-gigaword-200',\n"," 'glove-wiki-gigaword-300',\n"," 'glove-twitter-25',\n"," 'glove-twitter-50',\n"," 'glove-twitter-100',\n"," 'glove-twitter-200',\n"," '__testing_word2vec-matrix-synopsis']"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"xXIFlOLsybBm","executionInfo":{"status":"ok","timestamp":1628449580297,"user_tz":240,"elapsed":25103,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["# download and load the glove-wiki-gigaword-50\n","glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-JaJ10xvN-Gy"},"source":["### How to Use the Pre-trained Word Embeddings?"]},{"cell_type":"markdown","metadata":{"id":"NKXRgXAJOEg8"},"source":["Retrieve the embedding for a specific word"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLumJJlmOJCl","executionInfo":{"status":"ok","timestamp":1628450629447,"user_tz":240,"elapsed":255,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"c92feeaa-8c53-4211-b32f-c8cb5c809cda"},"source":["glove_vectors['atlanta']"],"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-1.0255  ,  1.14    ,  0.27088 ,  1.2964  , -0.22467 , -0.55808 ,\n","       -1.9727  , -0.52942 ,  0.55607 , -0.48596 , -0.7555  , -0.55799 ,\n","       -0.99334 ,  0.13091 ,  0.83527 , -0.058354, -0.79702 , -0.5973  ,\n","       -0.43055 ,  0.095148, -0.42831 ,  0.5277  , -0.41006 ,  0.64514 ,\n","       -0.59836 , -1.0417  , -0.060947, -0.45935 ,  0.79238 , -0.80473 ,\n","        1.781   ,  0.52496 ,  0.036867, -0.51445 , -0.19282 , -0.31396 ,\n","        0.094393,  0.17953 ,  0.91322 ,  0.48565 , -0.053946,  0.3348  ,\n","        0.24868 ,  0.71448 ,  0.040415,  1.4561  , -0.15356 , -0.15673 ,\n","       -0.55824 ,  1.0741  ], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QfYQezVCOK54","executionInfo":{"status":"ok","timestamp":1628450635870,"user_tz":240,"elapsed":301,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"30e52384-1294-432f-aa6f-a4b656cb459f"},"source":["glove_vectors['atlanta'].shape"],"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50,)"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"markdown","metadata":{"id":"_YK5R5sdOXtq"},"source":["Find the most similar words"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0qz1nOQbzJSD","executionInfo":{"status":"ok","timestamp":1628450263462,"user_tz":240,"elapsed":242,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"3857f88f-ac35-42ee-f432-175e47a7bec7"},"source":["glove_vectors.most_similar('atlanta')"],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('denver', 0.8360953330993652),\n"," ('houston', 0.833003044128418),\n"," ('dallas', 0.8196703195571899),\n"," ('seattle', 0.8138191103935242),\n"," ('austin', 0.808619499206543),\n"," ('miami', 0.8046270608901978),\n"," ('tampa', 0.7782285213470459),\n"," ('angeles', 0.7770278453826904),\n"," ('cincinnati', 0.7677363753318787),\n"," ('phoenix', 0.7639263868331909)]"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJ-66fBSz4yh","executionInfo":{"status":"ok","timestamp":1628449580299,"user_tz":240,"elapsed":19,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"941e13f8-5785-4dc0-e656-6134d62c3c27"},"source":["glove_vectors.most_similar('twitter')"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('facebook', 0.9333045482635498),\n"," ('myspace', 0.8801369667053223),\n"," ('youtube', 0.8430657982826233),\n"," ('blog', 0.8262056708335876),\n"," ('blogs', 0.8064823746681213),\n"," ('blogging', 0.7970671653747559),\n"," ('tumblr', 0.7901090383529663),\n"," ('email', 0.778261125087738),\n"," ('tweets', 0.7604536414146423),\n"," ('e-mail', 0.7538727521896362)]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"S4g3HM9RL2EU"},"source":["### Embeddings Capture Relational Meaning"]},{"cell_type":"code","metadata":{"id":"CmZDU7CxLGqr","executionInfo":{"status":"ok","timestamp":1628450038467,"user_tz":240,"elapsed":219,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["# calculate the cosine similarity between two vectors\n","def cosine_sim(a,b):\n","  return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9K0CeQytL0Jt","executionInfo":{"status":"ok","timestamp":1628450038873,"user_tz":240,"elapsed":3,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"89b2fed9-054c-4e76-ca3a-7a0a54eddb10"},"source":["cosine_sim(glove_vectors['king'],glove_vectors['queen'])"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7839044"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"fNmvAP2h0qsP","executionInfo":{"status":"ok","timestamp":1628450052413,"user_tz":240,"elapsed":227,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["a = glove_vectors['king'] - glove_vectors['man'] + glove_vectors['woman']"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gNA0Asz2LYsS","executionInfo":{"status":"ok","timestamp":1628450053234,"user_tz":240,"elapsed":222,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"d4b4be2d-ee61-49b1-8693-d909f8036813"},"source":["cosine_sim(a,glove_vectors['queen'])"],"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.86095816"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"fPyBqW2rMZFW"},"source":["Could you try another example shown in the lecture?\n","\n","e.g., `vector(‘paris’) - vector(‘france’) + vector(‘italy’)` and `vector(‘rome’)`"]},{"cell_type":"markdown","metadata":{"id":"y8NGLzcBQDCs"},"source":["# Sentence/Document Clustering with Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"hdI4UR9EQKS_"},"source":["Now that we know how to use pre-trained word embeddings, let's utilize the embeddings to cluster documents in 20-newsgroups."]},{"cell_type":"markdown","metadata":{"id":"pqem4fV0QXPB"},"source":["## Prepareing Data\n","\n","Similarly, the first step is to pre-process the input data: tokenization, lammatization and etc."]},{"cell_type":"code","metadata":{"id":"6FBT2et9QIkO","executionInfo":{"status":"ok","timestamp":1628451334849,"user_tz":240,"elapsed":61689,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["# Importing the needed packages\n","from nltk.tokenize import word_tokenize\n","\n","from nltk.corpus import stopwords\n","import string\n","from nltk.stem import WordNetLemmatizer\n","\n","# tokenization\n","tokenized_text = []\n","for sentence in text:\n","  tokenized_text.append(word_tokenize(sentence))\n","\n","# filtering stop words (numbers) and punctuations, and lemmatzing\n","stop_words = stopwords.words(\"english\")\n","stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'line', 'organization', 'university', 'wa', 'ha', \"'s\", \"n't\", \"'d\"])\n","\n","punctuations = string.punctuation  + \"*\" + \"/\" + \"\\\\\" + \"_\" + \"-\"\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","filtered_text = []\n","\n","for sent in tokenized_text:\n","  filtered_list = []\n","  for word in sent:\n","    # filter out tokens that have punctuations and numbers\n","      # word.isalpha() returns true if a string only contains letters.\n","    # filter out stop words\n","    if word.isalpha() and lemmatizer.lemmatize(word.lower()) not in stop_words and len(word) >= 2:\n","      filtered_list.append(lemmatizer.lemmatize(word.lower()))\n","  filtered_text.append(filtered_list)"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_n4AZQF7QniZ","executionInfo":{"status":"ok","timestamp":1628451340132,"user_tz":240,"elapsed":269,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"e5de1dcc-df72-4589-d2bb-b63a191665b8"},"source":["filtered_text[0]"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['lerxst',\n"," 'thing',\n"," 'car',\n"," 'maryland',\n"," 'college',\n"," 'park',\n"," 'wondering',\n"," 'anyone',\n"," 'could',\n"," 'enlighten',\n"," 'car',\n"," 'saw',\n"," 'day',\n"," 'sport',\n"," 'car',\n"," 'looked',\n"," 'late',\n"," 'early',\n"," 'called',\n"," 'bricklin',\n"," 'door',\n"," 'really',\n"," 'small',\n"," 'addition',\n"," 'front',\n"," 'bumper',\n"," 'separate',\n"," 'rest',\n"," 'body',\n"," 'know',\n"," 'anyone',\n"," 'tellme',\n"," 'model',\n"," 'name',\n"," 'engine',\n"," 'spec',\n"," 'year',\n"," 'production',\n"," 'car',\n"," 'made',\n"," 'history',\n"," 'whatever',\n"," 'info',\n"," 'funky',\n"," 'looking',\n"," 'car',\n"," 'please',\n"," 'thanks',\n"," 'il',\n"," 'brought',\n"," 'neighborhood',\n"," 'lerxst']"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"_SYspQnxQ3up"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dZYP-tNMQ86i"},"source":["## Generating Vector Representations of Sentences/Documents\n","\n","A common approach to vectorize a sentence/document is to use the average of all the vectors of words in the sentence/document.\n","\n","For words that are in the pre-trained word embedding models, we directly use the pre-trained embeddings. For words that are not in the pre-trained models, we call them **unknow words**. Usually, we will use a *zero vector* to represent them or just ignore them. If you want a more precise model that could cover those cases, you could train your own word2vec model over your own corpus. For more details, please refer to https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html and  https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92.\n"]},{"cell_type":"code","metadata":{"id":"_COk0TdKREHa","executionInfo":{"status":"ok","timestamp":1628452484526,"user_tz":240,"elapsed":219,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["def vectorize(doc, model):\n","  vectors = []\n","  \n","  # transform every token in the input doc to vectors\n","  for token in doc:\n","    #zero_vector = np.zeros(model.vector_size)\n","    if token in model:\n","      vectors.append(model[token])\n","    #else:\n","    #  vectors.append(zero_vector)\n","\n","  # average word vectors in one document\n","  vectors = np.asarray(vectors)\n","  avg_vec = vectors.mean(axis=0)\n","\n","  return avg_vec"],"execution_count":87,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O6UtCAdESHSB","executionInfo":{"status":"ok","timestamp":1628452485508,"user_tz":240,"elapsed":299,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"865bd030-554b-48e8-d493-ee6c1563b794"},"source":["vectorize(filtered_text[0], glove_vectors)"],"execution_count":88,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.1752914 ,  0.13909456,  0.22094107, -0.12333324,  0.23647927,\n","       -0.04230335, -0.63693875, -0.14860386, -0.06012844, -0.03337453,\n","       -0.07830777, -0.05556682, -0.4236353 , -0.04059649,  0.39091134,\n","        0.1679264 , -0.11425725,  0.33666366, -0.1622146 , -0.52783036,\n","        0.05852604,  0.16622783, -0.16341592,  0.23142605,  0.22687714,\n","       -1.2628195 , -0.48308963,  0.18888098,  0.35635024, -0.24269347,\n","        2.4111907 ,  0.14718468, -0.18373367, -0.14261353,  0.22068126,\n","        0.06201767,  0.15856849,  0.07977802, -0.03621079, -0.17915004,\n","       -0.13333684, -0.00876729, -0.10287292, -0.00261767,  0.14338624,\n","        0.1158571 , -0.0657627 , -0.21478257,  0.1238562 ,  0.06324318],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"code","metadata":{"id":"KNJZs6-MSIcb","executionInfo":{"status":"ok","timestamp":1628452490758,"user_tz":240,"elapsed":4422,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["# vectorize all the news\n","\n","vectorized_text = []\n","for doc in filtered_text:\n","  vectorized_text.append(vectorize(doc, glove_vectors))\n"],"execution_count":89,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1RAn4pDTTghs"},"source":["## Clustering Documents with KMeans Cluster through Sklearn"]},{"cell_type":"code","metadata":{"id":"G0exKmBtT4rH","executionInfo":{"status":"ok","timestamp":1628452981959,"user_tz":240,"elapsed":5271,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}}},"source":["from sklearn.cluster import KMeans\n","km = KMeans(n_clusters=20, random_state = 100).fit(vectorized_text)"],"execution_count":112,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XEXfA5pGUgO-","executionInfo":{"status":"ok","timestamp":1628453511469,"user_tz":240,"elapsed":276,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"c76b7795-37c2-4ddd-9aef-777f92d9c815"},"source":["# view the cluster assigned to a given document, e.g., the first document:\n","km.labels_[0]"],"execution_count":135,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":135}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hsWjNPtTY_-T","executionInfo":{"status":"ok","timestamp":1628453494797,"user_tz":240,"elapsed":251,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"2f9d08f6-843d-46d0-9701-820b972f040e"},"source":["# view the center of a given cluster. For example, the center of cluster 1:\n","km.cluster_centers_[1]"],"execution_count":133,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.05587016,  0.0814233 ,  0.1851054 , -0.05953059,  0.11577202,\n","        0.08937234, -0.35560544, -0.13192832,  0.00598951, -0.02030215,\n","       -0.01738443,  0.0956945 , -0.27545068,  0.04817316,  0.21985977,\n","        0.15796056,  0.00575328,  0.12508632, -0.20474372, -0.49394582,\n","        0.01072044,  0.04175672,  0.01448295,  0.06068192,  0.1620056 ,\n","       -1.01463056, -0.25587663,  0.15912723,  0.32359527, -0.19843669,\n","        2.06408551,  0.08222181, -0.12588641, -0.0612789 ,  0.08557905,\n","        0.00979518,  0.11233084,  0.10292808,  0.08265812, -0.15084695,\n","        0.01966269,  0.04889586, -0.03501873,  0.12977324,  0.03491078,\n","        0.04244387,  0.06805984, -0.10252202,  0.02710781,  0.12603377])"]},"metadata":{"tags":[]},"execution_count":133}]},{"cell_type":"markdown","metadata":{"id":"vxCuymESXPUA"},"source":["## Interpreting Clusters\n","\n","To interpret clusters, we usually find several most repreentative documents in one cluster and try to summarize the topics from them."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AQESGisUs2E","executionInfo":{"status":"ok","timestamp":1628453147175,"user_tz":240,"elapsed":256,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"7b262393-33f1-4358-adf3-cdd96f51ff4d"},"source":["# For example, let's take a look at the most representative docs in cluster 1\n","most_representative_docs = np.argsort(\n","    np.linalg.norm(vectorized_text - km.cluster_centers_[1], axis=1)\n",")\n","for d in most_representative_docs[:2]:\n","    print(text[d])\n","    print(\"\\n ================================= \\n\")\n","    "],"execution_count":124,"outputs":[{"output_type":"stream","text":["From: kking@cs.uah.edu (Ken King)\n","Subject: Re: The Kuebelwagen??!!          \n","Reply-To: kking@uahcs2.uah.edu (Ken King)\n","Organization: Computer Science Dept. - Univ. of Alabama in Huntsville\n","Lines: 36\n","\n","In article <C5K5Co.F09@mentor.cc.purdue.edu> thwang@mentor.cc.purdue.edu (Tommy Hwang) writes:\n",">\tSorry for the mis-spelling, but I forgot how to spell it after \n",">my series of exams and NO-on hand reference here.\n",">\n",">\tIs it still possible to get those cute WWII VW Jeep-wanna-be's?\n",">A replica would be great I think.  \n","\n","  greetings:\n","  you may be in luck.  i seem to recall seeing a blurb in one of\n","the kit car magazines about a company in norway who pulled a\n","mould (sp?) off a real kubel, and has adapted it to the beetle\n","floorpan.  as for the suspension, all i can remember about the\n","vw thing i used to own is that it had about 3\" more suspension\n","travel than a stock beetle, but i'd heard that there were after-\n","market parts for off-road use that were as good or better.  note\n","that the major difference (looks wise) between a kubel & a thing\n","are the hood and the fenders.  the kubel had an external spare\n","mounted *on* the hood, and the hood sloped down (for visibility?)\n","sharply, and had rounded fenders.  the thing has a lightly sloped \n","hood with the spare mounted inside (unless moved to make for more\n","luggage space...) and has half-hexagon shaped fenders (imagine a\n","nut large enough to put a tire *in*, and cut off the bottom half\n","of it...).\n","  unfortunately, i don't have that info anymore.  try stopping\n","at a local bookstore and copying down the phone numbers for the\n","two big mag's and calling them.  they might be able to get the\n","number for you (don't forget to calculate the time difference to\n","norway before calling...).\n","\n","later,\n","kc\n","-- \n","          ___==A==___          | Quick Bones, help me get | #include \n","  .---====   ( o )   ====---.  | this Klingon off my *ss! |  <std/disclaimer.h>\n"," /        ~~~~~~~~~~~        \\ | Damn it, Jim, I'm a      | \n"," ()     kking@cs.uah.edu    () | doctor, not a bidet!  :) | \n","\n","\n"," ================================= \n","\n","From: laszlo@eclipse.cs.colorado.edu (Laszlo Nemeth)\n","Subject: Re: BMW MOA members read this!\n","Nntp-Posting-Host: eclipse.cs.colorado.edu\n","Organization: University of Colorado Boulder, Pizza Disposal Group\n","Lines: 27\n","\n","In article <1993Apr19.193331.11327@sarvax.cmhnet.org>, frog@sarvax.cmhnet.org (Jeff 'Frog' Campbell) writes:\n","|> From article <C5px3n.Kw0@murdoch.acc.Virginia.EDU>, by cdw2t@dayhoff.med.Virginia.EDU (Dances With Federal Rangers):\n","|> > \n","|> > ObMotoWashing: Is it just me, or does everyone cut their finger(s) on the\n","|> > Evil Cotterpin (tm), lurking somewhere in the dark recesses of the back end\n","|> > of the bike, when giving the prized moto a bath?  I seem to slice the pinkie\n","|> > of one hand or the other *every* time (*both* of them this time!).\n","|> \n","|> It's you. Beemers have no EC (tm).\n","\n","OH yes they do! but considering i never wash my BMW (unless i need\n","to work on it) i never get cuts untill the tools come out.....\n","altho the best scar (now faded) was from the exhaust piper,\n","while try to change the oil, after overheating the engine (watch\n","out for idiot drivers that won't let you pass in the mountains\n","when you have a rider, case of oil (it was on sale), and case of \n","beer), with a drain bolt that decided to seize.\n","\n","just how does everyone else clean out the area under the transmission\n","on a BMW R bike? they only way i have found is to remove the\n","engine and transmission. that and the clutch arm are impossible to\n","clean (which is wear one of the EC (s&m) are located).\n","\n","\n","laz\n","Ps anyone know where i can get the heads polished and ported\n","cheap. also how much should that run.\n","\n","\n"," ================================= \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yXP6UlfuVDMs","executionInfo":{"status":"ok","timestamp":1628453279363,"user_tz":240,"elapsed":245,"user":{"displayName":"Jiaao Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjUCQMroRzHG3EnPAZN5SyYw0RfkKUVUh9qzYxZ=s64","userId":"08215740097147588674"}},"outputId":"a58c6465-4930-4cb2-b934-a5a9efb4000a"},"source":["# For example, let's take a look at the most representative docs in cluster 2\n","most_representative_docs = np.argsort(\n","    np.linalg.norm(vectorized_text - km.cluster_centers_[2], axis=1)\n",")\n","for d in most_representative_docs[:2]:\n","    print(text[d])\n","    print(\"\\n ================================= \\n\")"],"execution_count":132,"outputs":[{"output_type":"stream","text":["From: pat@rwing.UUCP (Pat Myrto)\n","Subject: Re: White House Public Encryption Management Fact Sheet\n","Article-I.D.: rwing.2087\n","Distribution: na\n","Organization: Totally Unorganized\n","Lines: 52\n","\n","In article <19APR199313020883@charon.gsfc.nasa.gov> paul@charon.gsfc.nasa.gov (Paul Olson) writes:\n",">In article <1qnav4$r3l@transfer.stratus.com>, cme@ellisun.sw.stratus.com (Carl Ellison) writes...\n",">>In article <C5LGAz.250@dove.nist.gov> clipper@csrc.ncsl.nist.gov (Clipper Chip Announcement) writes:\n",">> \n",">>>Further, the Attorney General\n",">>\n","> [ ... good post describing what is in store for us deleted ... ]\n",">\n",">It's also interesting to note that two months ago Rush Limbaugh said that\n",">Clinton would have the \"plumbers\" out in force shortly.  Clinton and his\n",">henchmen firmly believe in strong ubiquitous government control.  Anytime a\n",">leader believes in that, the leader will use every means possible to retain\n",">that control and take more.\n",">\n",">WE have to take OUR government back.  Otherwise we will end up living in the\n",">equivalent of a high-tech third world dictatorship.  We have to take\n",">responsibility for ourselves, our personal welfare, and our actions.\n","\n","I totally agree.  But how do you propose we take government back?  They\n","obviously don't listen to the people or want the people to know who is\n","responsibile for what (a person telnetted the site of the Clipper chip\n","release, to see what the entity 'clipper' was, and got a few lists.\n","BUt when another person tried a bit later, the commands were disabled)\n","Does not sound like an Administration that wants to have any accountability\n","or information they don't control given to the people.  The secret\n","development and implimentation of the Clipper Chip decision further\n","backs that up.  You can bet unaurhorized encryption methods and software\n","will be considered 'terrorist tools' and also subject to civil forfeiture,\n","along with the systems that are running it.  YOU WATCH, SEE IF I AM WRONG.\n","\n","The government is not going to be very cooperative about the people taking\n","it back.  And they have all the resources, unlimited access to the media\n","for propeganda, and almost all the guns (soon to be ALL the guns if\n","Clinton's agenda succeeds)...   Those that do not play ball?  Waco\n","might be a good example of what to expect...  The warrant (just released)\n","stated the reason for the raid was the BDs spent a very large sum\n","for weapons, over an undetermined amount of time.  I don't recall\n","spending a lot of money on guns, etc being illegal ... yet, that is.\n","\n","Clinton might go down in history as the worst thing to ever happen to\n","the US of A. ... Now to be known as the 'Peoples Socalist Democratic\n","Republic of America'  (PSDRA).\n","\n","Big Brother is LISTENING!!!\n","\n","Hail Big Brother... (and Sister...?)  only ten years late!!!\n","\n","-- \n","pat@rwing.uucp      [Without prejudice UCC 1-207]     (Pat Myrto) Seattle, WA\n","         If all else fails, try:       ...!uunet!pilchuck!rwing!pat\n","WISDOM: \"Only two things are infinite; the universe and human stupidity,\n","         and I am not sure about the former.\"              - Albert Einstien\n","\n","\n"," ================================= \n","\n","From: artmel@well.sf.ca.us (Arthur Melnick)\n","Subject: Big Brother (Clipper) chip\n","Summary: Some thoughts on the use of the Big Brother (Clipper) chip\n","Keywords: clipper\n","Nntp-Posting-Host: well.sf.ca.us\n","Organization: The Whole Earth 'Lectronic Link, Sausalito, CA\n","Lines: 57\n","\n","\n","     There are some issues which come to mind when one considers\n","the law enforcement aspects of the use of the Big Brother\n","(Clipper) chip.\n","     The drug dealers and terrorists aren't going to let\n","themselves be caught by using this type of encryption.  In 1985\n","the New York Times reported that government investigators broke\n","up a narcotics ring that was operating highly sophisticated\n","equipment capable of allowing the leaders of the ring to\n","eavesdrop on the law-enforcement agents who were trying to arrest\n","them.\n","     A Mr. Deely, an NSA official, said \"There are a lot of\n","medium-sized countries that would have been proud to have the\n","signals intelligence operation of this group.\"\n","     For every John Gotti there are probably many more people who\n","have the sophistication to know what the risks of unsecure\n","communications are.  The press given to the Big Brother chip will\n","only increase their numbers.\n","     Even if there is some benefit to law-enforcement through the\n","use of Big Brother, it must be weighed against the constitutional\n","and civil liberties questions involved.\n","     For example, in some areas of the world torture is used as\n","an investigative tool by the local \"law-enforcement\" people.  I\n","suspect it is an effective means of obtaining information and\n","shortening many investigations.  It probably also helps keep the\n","conviction rate high.\n","     The fact that the torture tool is not used in this country\n","(even with a court order obtained by showing \"probable cause\") is\n","because we have rightly balanced the questions of expediency and\n","what is ethically and morally right.\n","     I think that the same question of expediency versus morality\n","should come into play when considering the use of Big Brother.  I\n","vote for morality.\n","     I am quite disturbed by what I interpret as a veiled threat\n","to prohibit the use of all encryption if this Big Brother chip is\n","not put into wide spread use.  After a quick reading of the White\n","House press release I came away with that impression.\n","     To most of the American public, the word \"hacker\" has\n","rightly or wrongly come to mean \"high tech adolescent vandal\".\n","It has struck me that most of the people posting to sci.crypt\n","regarding this issue are intelligent, thoughtful individuals who\n","have genuine concerns about the privacy and constitutional issues\n","surrounding Big Brother.  I hope that the use of Big Brother does\n","not become mandatory and other encryption become illegal.  I\n","would hate to see this become some kind of high tech Volstead\n","Act.\n","     The high speed digital communications revolution is coming\n","at us with the speed of an SST.  The times they are a changing,\n","and just as IBM is learning that they can't do business the same\n","way they have done it for the past 40 years, maybe NSA should\n","evaluate another approach.\n","     EFF, who have correctly questioned the cryptographic\n","strength of Big Brother, may need to send a stronger message out\n","regarding the constitutional issues involved.\n","     Al Gore may want to think this one through a little more.\n","     And as for Dorothy Elizabeth Robling Denning: En quoi cela\n","vous concerne, cheri?\n","\n","\n"," ================================= \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5ltBVizKYZ3E"},"source":["### Practice:\n","\n","Can you use similar technique to interpret and summarize other clsuters? Are they consistent with the Topic Modeling from LDA? Which one do you think is better?"]},{"cell_type":"code","metadata":{"id":"qA0JojJVbb_y"},"source":[""],"execution_count":null,"outputs":[]}]}