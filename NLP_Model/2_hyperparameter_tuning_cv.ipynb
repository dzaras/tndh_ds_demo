{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee64415",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (Cross Validation)\n",
    "\n",
    "#### Goal:\n",
    "To determine optimized hyperparameters for SVM, random forest, and gradient boosted trees classifiers\n",
    "\n",
    "#### Input(s):\n",
    "\n",
    "CSV file with preprocessed narrative text ({my_directory}/processed_narr_{batch_date}.csv)\n",
    "File with case/control flags ('E:/Data Science Demonstration Project/SUDORSdataTruth.xlsx')\n",
    "#### Output(s):\n",
    "\n",
    "CSV file with tuned hyperparameters for each model ({my_directory}/tuned_params_{batch_date}.pickle)\n",
    "#### To run, set 2 variables and make sure correct input files are specified in first cell:\n",
    "\n",
    "my_directory = where you want outputs to save (e.g., 'C:/Users/dc20b49/Documents/TDH_DS_Demo/')\n",
    "batch_date = batch_date used in 1_preprocessing_pipeline.ipynb (e.g, '8-4-22')\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c99cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_directory = C:/Users/dc20b46/Desktop/tndh_ds_demo/,\n",
      "batch_date = 8-10-22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "my_directory = 'C:/Users/dc20b46/Desktop/tndh_ds_demo/'\n",
    "batch_date = '8-10-22'\n",
    "\n",
    "print(f'''my_directory = {my_directory},\n",
    "batch_date = {batch_date}''')\n",
    "\n",
    "# load data\n",
    "narr_df = pd.read_csv(f'{my_directory}/processed_narr_{batch_date}.csv')\n",
    "print(narr_df.shape)\n",
    "\n",
    "# assign case-control flags\n",
    "cases = pd.read_csv(f'{my_directory}/')\n",
    "cases = pd.read_excel('T:/Data Science Demonstration Project/SUDORSdataTruth.xlsx') \n",
    "print(cases.shape)\n",
    "\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a65d1",
   "metadata": {},
   "source": [
    "my_directory = C:/Users/dc20b46/Documents/tndh_ds_ddemo/,\n",
    "batch_date = 8-10-22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09770114",
   "metadata": {},
   "source": [
    "### Prepare Test and Train Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def transform_data(count_matrix, vectorizer, idx):\n",
    "    count_array = count_matrix.toarray()\n",
    "    df_transformed = pd.DataFrame(data=count_array, columns = vectorizer.get_feature_names_out())\n",
    "    df_transformed = df_transformed[[c for c in df_transformed.columns if not re.match(r'^\\d+$', c) and len(c) > 3]]\n",
    "    df_transformed.set_index(idx, inplace = True)\n",
    "    return df_transformed\n",
    "\n",
    "# assign case/control flags\n",
    "narr_df['DID'] = narr_df['DID'].astype(str)\n",
    "cases['DID'] = cases['DID'].astype(str)\n",
    "narr_df['case'] = narr_df['DID'].isin(cases.DID.unique()).astype(int)\n",
    "\n",
    "# remove autopsies < 100 characters\n",
    "print(f'''Autopsies removed: {np.sum(narr_df.full_narr_lemma_text_len < 100)} ''')\n",
    "narr_df = narr_df.loc[narr_df.full_narr_lemma_text_len >= 100]\n",
    "\n",
    "# set test/train data\n",
    "narr_df['train'] = narr_df['year'].apply(lambda x: x < 2021).astype(int)\n",
    "narr_df.set_index('DID', inplace = True)\n",
    "\n",
    "# shuffle data\n",
    "shuffled_df = shuffle(narr_df, random_state = 0)\n",
    "\n",
    "# DIDs for train, calibration, and test sets\n",
    "all_train_DID = shuffled_df.loc[shuffled_df.train==1].index\n",
    "train_DID, calib_DID = train_test_split(all_train_DID, test_size = 0.05, random_state = 0)\n",
    "test_DID = shuffled_df.loc[shuffled_df.train==0].index\n",
    "\n",
    "# get labels\n",
    "ytrain, _, ytest = shuffled_df['case'].loc[train_DID], shuffled_df['case'].loc[calib_DID], shuffled_df['case'].loc[test_DID]\n",
    "\n",
    "print(f'''Total: {shuffled_df.shape[0]}\n",
    "Train: {len(train_DID)}\n",
    "[Calibration: {len(calib_DID)}]\n",
    "Test: {len(test_DID)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5144e15b",
   "metadata": {},
   "source": [
    "Autopsies removed: 146 \n",
    "Total: 17375\n",
    "Train: 10240\n",
    "Calibration: 539\n",
    "Test: 6596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### term frequency\n",
    "# # create vocabulary based on training set\n",
    "# coun_vect = CountVectorizer(min_df = 20)\n",
    "# count_matrix = coun_vect.fit_transform(shuffled_df.loc[train_DID]['full_narr_lemma_text'])\n",
    "# word_count_train = transform_data(count_matrix, coun_vect, train_DID)\n",
    "\n",
    "# # calibration\n",
    "# count_matrix_calib = coun_vect.transform(shuffled_df.loc[calib_DID]['full_narr_lemma_text'])\n",
    "# word_count_calib = transform_data(count_matrix_calib, coun_vect, calib_DID)\n",
    "\n",
    "# # test\n",
    "# count_matrix_test = coun_vect.transform(shuffled_df.loc[test_DID]['full_narr_lemma_text'])\n",
    "# word_count_test = transform_data(count_matrix_test, coun_vect, test_DID)\n",
    "\n",
    "### TF-IDF\n",
    "# create vocabulary based on training set\n",
    "tfidf_vect = TfidfVectorizer(min_df=20)\n",
    "tfidf_matrix = tfidf_vect.fit_transform(shuffled_df.loc[train_DID]['full_narr_lemma_text'])#.values.astype('U'))\n",
    "tfidf_train = transform_data(tfidf_matrix, tfidf_vect, train_DID)\n",
    "\n",
    "# calibration\n",
    "# tfidf_matrix_calib = tfidf_vect.transform(shuffled_df.loc[calib_DID]['full_narr_lemma_text'])\n",
    "# tfidf_calib = transform_data(tfidf_matrix_calib, tfidf_vect, calib_DID)\n",
    "\n",
    "# test\n",
    "tfidf_matrix_test = tfidf_vect.transform(shuffled_df.loc[test_DID]['full_narr_lemma_text'])\n",
    "tfidf_test = transform_data(tfidf_matrix_test, tfidf_vect, test_DID)\n",
    "\n",
    "# datasets\n",
    "datasets = {\n",
    "    #'Freq': [word_count_train, word_count_calib, word_count_test],\n",
    "            'TFIDF': [tfidf_train, tfidf_calib, tfidf_test]}\n",
    "\n",
    "xtrain, _, xtest = datasets['TFIDF']\n",
    "\n",
    "print(f'''\n",
    "Vocabulary: {tfidf_train.shape}\n",
    "Total: {shuffled_df.shape[0]}\n",
    "Train: {xtrain.shape[0]}\n",
    "Test: {xtest.shape[0]}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578b12c",
   "metadata": {},
   "source": [
    "Vocabulary: (10240, 4006)\n",
    "Total: 17375\n",
    "Train: 10240\n",
    "Test: 6596"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739a4a58",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17369243",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'gamma': [0.1, 0.01, 0.001, 0.0001],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "clf = GridSearchCV(\n",
    "    estimator=svm.SVC(),\n",
    "    param_grid=params,\n",
    "    cv=5,\n",
    "    n_jobs=5,\n",
    "    verbose=1\n",
    ")\n",
    "clf.fit(xtrain, ytrain)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52073740",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
    "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
    "Wall time: 37min 59s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e86c2",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8863b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Create the random grid of hyperparameters\n",
    "parameter_grid = {'n_estimators': [250, 275, 300, 325, 350, 375, 400, 425, 450],\n",
    "                  'max_depth': [35, 37, 39, 41, 43, 45, 47],\n",
    "                  'min_samples_split': [2, 3, 4, 5, 6, 7, 8],\n",
    "                  'min_samples_leaf': list(np.arange(1, 3)),\n",
    "                  'oob_score':  [True]}\n",
    "\n",
    "def randomforest_cv(X_train, y_train, parameter_grid = parameter_grid):\n",
    "    \n",
    "    clf = RandomForestClassifier()\n",
    "\n",
    "    rf_cf = RandomizedSearchCV(clf, parameter_grid, \n",
    "                                   n_iter=50, cv=5, n_jobs=6)\n",
    "\n",
    "    rf_cf.fit(X_train, y_train)\n",
    "    \n",
    "    return rf_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10bf911",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# train models\n",
    "rfc_cv = randomforest_cv(xtrain, ytrain)\n",
    "\n",
    "print(rfc_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8ed6e4",
   "metadata": {},
   "source": [
    "RandomForestClassifier(max_depth=43, min_samples_split=6, n_estimators=450,\n",
    "                       oob_score=True)\n",
    "Wall time: 48min 20s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc340962",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998d996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def xgbc_CV(xgb_params, param_grid, xtrain, ytrain):\n",
    "    \n",
    "    min_error = np.inf\n",
    "    best_params = None\n",
    "    \n",
    "    dtrain = xgb.DMatrix(xtrain, label=ytrain)\n",
    "\n",
    "    for max_depth, min_child_weight, eta, alpha, lambda_val in param_grid:\n",
    "\n",
    "        # Update our parameters\n",
    "        xgb_params['max_depth'] = max_depth\n",
    "        xgb_params['min_child_weight'] = min_child_weight\n",
    "        xgb_params['eta'] = eta\n",
    "        xgb_params['reg_alpha'] = alpha\n",
    "        xgb_params['reg_lambda'] = lambda_val\n",
    "\n",
    "        # Run CV\n",
    "        cv_results = xgb.cv(\n",
    "            xgb_params,\n",
    "            dtrain,\n",
    "            num_boost_round=50,\n",
    "            nfold=5,\n",
    "            metrics={'error'},\n",
    "            early_stopping_rounds=3\n",
    "        )\n",
    "        \n",
    "        # Update best score\n",
    "        mean_error = cv_results['test-error-mean'].min()\n",
    "        boost_rounds = cv_results['test-error-mean'].argmin()\n",
    "        #print(\"\\tError {} for {} rounds\".format(mean_error, boost_rounds))\n",
    "        if mean_error < min_error:\n",
    "            min_error = mean_error\n",
    "            best_params = (max_depth, min_child_weight, eta, alpha, lambda_val)\n",
    "\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9131db2",
   "metadata": {},
   "source": [
    "Wall time: 617 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import itertools\n",
    "\n",
    "# random grid\n",
    "max_depths = [3, 4, 5, 6, 7]\n",
    "min_child_wghts = [2, 3, 4, 5]\n",
    "etas = [0.1, 0.25, 0.5]\n",
    "alphas = [0, 0.001, 0.01, 0.1, 0.2, 0.25, 0.3, 0.4]\n",
    "lambdas = [0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\n",
    "hyp_params = [max_depths,\n",
    "              min_child_wghts,\n",
    "              etas,\n",
    "              alphas,\n",
    "              lambdas]\n",
    "\n",
    "param_grid = list(itertools.product(*hyp_params))\n",
    "\n",
    "# parameters\n",
    "xgb_params = {\n",
    "        'max_depth': 5,\n",
    "        'eta': 0.25,\n",
    "        'objective': 'binary:hinge',\n",
    "        'eval_metric': 'error',\n",
    "        'colsample_bytree': 0.5,\n",
    "        'reg_alpha' : 0,\n",
    "        'reg_lambda' : 0\n",
    "}\n",
    "\n",
    "print('Performing cross validation...')\n",
    "\n",
    "# hyperparameters of best model\n",
    "best_params = xgbc_CV(xgb_params, param_grid, xtrain, ytrain)\n",
    "\n",
    "# best params\n",
    "max_depth, min_child_weight, eta, alpha, lambda_val = best_params\n",
    "\n",
    "xgb_params['max_depth'] = max_depth\n",
    "xgb_params['min_child_weight'] = min_child_weight\n",
    "xgb_params['eta'] = eta\n",
    "xgb_params['reg_alpha'] = alpha\n",
    "xgb_params['reg_lambda'] = lambda_val\n",
    "\n",
    "xgb_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d182d03e",
   "metadata": {},
   "source": [
    "{'max_depth': 4,\n",
    " 'eta': 0.25,\n",
    " 'objective': 'binary:hinge',\n",
    " 'eval_metric': 'error',\n",
    " 'colsample_bytree': 0.5,\n",
    " 'reg_alpha': 0,\n",
    " 'reg_lambda': 0.2,\n",
    " 'min_child_weight': 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9b937",
   "metadata": {},
   "source": [
    "## Save Tuned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acccdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tuned_params = {'SVM': clf.best_params_,\n",
    "                'RF': rfc_cv.best_params_,\n",
    "                'XGB': xgb_params}\n",
    "\n",
    "with open(f'{my_directory}/tuned_params_{batch_date}.pickle', 'wb') as handle:\n",
    "    pickle.dump(tuned_params, handle)\n",
    "    \n",
    "print(f'Tuned parameters saved to {my_directory}/tuned_params_{batch_date}.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
