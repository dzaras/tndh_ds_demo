{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e0e247",
   "metadata": {},
   "source": [
    "## Model Evaluation (Bootstrapped Samples)\n",
    "#### Goal:\n",
    "To evaluate model discrimination and calibration of tuned SVM, random forest, and gradient boosted trees classifiers and logistic regression model\n",
    "\n",
    "#### Input(s):\n",
    "\n",
    "CSV file with preprocessed narrative text (processed_narr_{batch_date}.csv, output of run_preprocessing.ipynb)\n",
    "File with case/control flags ('E:/Data Science Demonstration Project/SUDORSdataTruth.xlsx')\n",
    "File with tuned hyperparameters for each model (tuned_params_{batch_date}.pickle, output of hyperparameter_tuning.ipynb)\n",
    "#### Output(s):\n",
    "\n",
    "95% CIs for each discrimination and calibration metric (saved in file)\n",
    "SVM, RF, and XGB models achieving median discrimination\n",
    "Top SHAP values for median random forest and gradient boosted trees classifiers (saved in file)\n",
    "SHAP plots for top 20 features\n",
    "#### To run, set 2 variables and make sure correct input files are specified in first cell:\n",
    "\n",
    "my_directory = where you want outputs to save (e.g., 'C:/Users/dc20b49/Documents/TDH_DS_Demo/')\n",
    "batch_date = batch_date used in 1_preprocessing_pipeline.ipynb (e.g, '8-4-22')\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "my_directory = 'C:/Users/dc20b46/Documents/tndh_ds_demo/'\n",
    "batch_date = '8-10-22'\n",
    "\n",
    "print(f'''my_directory = {my_directory},\n",
    "batch_date = {batch_date}''')\n",
    "\n",
    "### load data\n",
    "# tuned hyperparameters\n",
    "# with open(f'{my_directory}/tuned_params_{batch_date}.pickle', 'rb') as handle:\n",
    "#     tuned_params = pickle.load(handle)\n",
    "tuned_params = {\n",
    "    'RF': {'max_depth': 37, 'min_samples_leaf': 1, \n",
    "           'min_samples_split': 4, 'n_estimators': 350, 'oob_score': True},\n",
    "    'XGB': {'max_depth': 4, 'min_child_weight': 2, 'eta': 0.25,\n",
    "            'reg_alpha': 0.01, 'reg_lambda': 0.25, 'objective': 'binary:hinge',\n",
    "            'eval_metric': 'error', 'colsample_bytree': 0.5},\n",
    "    'SVM': {'kernel': 'rbf', 'C': 10, 'gamma': 0.1}\n",
    "}\n",
    "\n",
    "# preprocessed narratives\n",
    "narr_df = pd.read_csv(f'{my_directory}/processed_narr_{batch_date}.csv')\n",
    "print(narr_df.shape)\n",
    "\n",
    "# case-control flags\n",
    "#cases = pd.read_csv(f'{my_directory}/')\n",
    "cases = pd.read_excel('E:/Data Science Demonstration Project/SUDORSdataTruth.xlsx')\n",
    "print(cases.shape)\n",
    "\n",
    "print('Data loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab81f6",
   "metadata": {},
   "source": [
    "## Prepare Test and Train Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007baaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import re\n",
    "\n",
    "def transform_data(count_matrix, vectorizer, idx):\n",
    "    count_array = count_matrix.toarray()\n",
    "    df_transformed = pd.DataFrame(data=count_array, columns = vectorizer.get_feature_names_out())\n",
    "    df_transformed = df_transformed[[c for c in df_transformed.columns if not re.match(r'^\\d+$', c) and len(c) > 3]]\n",
    "    df_transformed.set_index(idx, inplace = True)\n",
    "    return df_transformed\n",
    "\n",
    "# assign case/control flags\n",
    "narr_df['case'] = narr_df['DID'].isin(cases.DID.unique()).astype(int)\n",
    "\n",
    "# remove autopsies < 100 characters\n",
    "print(f'''Autopsies removed: {np.sum(narr_df.full_narr_lemma_text_len < 100)} ''')\n",
    "narr_df = narr_df.loc[narr_df.full_narr_lemma_text_len >= 100]\n",
    "\n",
    "# set test/train data\n",
    "narr_df['train'] = narr_df['year'].apply(lambda x: x < 2021).astype(int)\n",
    "narr_df.set_index('DID', inplace = True)\n",
    "\n",
    "# shuffle data\n",
    "shuffled_df = shuffle(narr_df, random_state = 0)\n",
    "\n",
    "# DIDs for train, calibration, and test sets\n",
    "all_train_DID = shuffled_df.loc[shuffled_df.train==1].index\n",
    "train_DID, calib_DID = train_test_split(all_train_DID, test_size = 0.05, random_state = 0)\n",
    "test_DID = shuffled_df.loc[shuffled_df.train==0].index\n",
    "\n",
    "# get labels\n",
    "ytrain, ycalib, ytest = shuffled_df['case'].loc[train_DID], shuffled_df['case'].loc[calib_DID], shuffled_df['case'].loc[test_DID]\n",
    "\n",
    "### TF-IDF\n",
    "# create vocabulary based on training set\n",
    "tfidf_vect = TfidfVectorizer(min_df=20)\n",
    "tfidf_matrix = tfidf_vect.fit_transform(shuffled_df.loc[train_DID]['full_narr_lemma_text'].values.astype('U'))\n",
    "tfidf_train = transform_data(tfidf_matrix, tfidf_vect, train_DID)\n",
    "\n",
    "# calibration\n",
    "tfidf_matrix_calib = tfidf_vect.transform(shuffled_df.loc[calib_DID]['full_narr_lemma_text'])\n",
    "tfidf_calib = transform_data(tfidf_matrix_calib, tfidf_vect, calib_DID)\n",
    "\n",
    "# test\n",
    "tfidf_matrix_test = tfidf_vect.transform(shuffled_df.loc[test_DID]['full_narr_lemma_text'])\n",
    "tfidf_test = transform_data(tfidf_matrix_test, tfidf_vect, test_DID)\n",
    "\n",
    "# datasets\n",
    "datasets = {'TFIDF': [tfidf_train, tfidf_calib, tfidf_test]}\n",
    "\n",
    "print(f'''\n",
    "Vocabulary: {tfidf_train.shape}\n",
    "Total: {shuffled_df.shape[0]}\n",
    "Train: {tfidf_train.shape[0]}\n",
    "Calibration: {tfidf_calib.shape[0]}\n",
    "Test: {tfidf_test.shape[0]}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac49da3",
   "metadata": {},
   "source": [
    "## Train Models on Bootstrapped Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410aa099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap samples\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, fbeta_score\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "def bs_samples(x, y):\n",
    "    rand_idx = np.random.choice(x.index, replace = True, size = x.shape[0])\n",
    "    x_bs, y_bs = x.loc[rand_idx], y.loc[rand_idx]\n",
    "    return x_bs, y_bs\n",
    "\n",
    "def get_metrics(method, mod, X_test, y_test):\n",
    "    preds = mod.predict(X_test)\n",
    "    AUROC = roc_auc_score(y_test, preds)\n",
    "    precision = precision_score(y_test, preds)\n",
    "    recall = recall_score(y_test, preds)\n",
    "    f1= f1_score(y_test, preds)\n",
    "    f2 = fbeta_score(y_test, preds, beta=2)\n",
    "    accuracy = mod.score(X_test, y_test)\n",
    "    return [method, AUROC, precision, recall, f1, f2, accuracy]\n",
    "\n",
    "xtrain, xcalib, xtest = datasets['TFIDF']\n",
    "\n",
    "print(f'''\n",
    "Train: {xtrain.shape[0]}\n",
    "Calibration: {xcalib.shape[0]}\n",
    "Test: {xtest.shape[0]}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1233907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "models = {}\n",
    "models['Logistic'], models['SVM'], models['RF'], models['XGB'] = {}, {}, {}, {}\n",
    "for i in range(1):\n",
    "    print(f'Starting boostrap #{i+1}')\n",
    "    \n",
    "    # sample from train data\n",
    "    xtrain_bs, ytrain_bs = bs_samples(xtrain, ytrain)\n",
    "    \n",
    "    ### fit models\n",
    "    # logistic regression\n",
    "    print(f'Training logistic regression model...')\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(xtrain_bs, ytrain_bs)\n",
    "    models['Logistic'].update({i: lr})\n",
    "  \n",
    "    # svm\n",
    "    print(f'Training SVM model...')\n",
    "    svm_mod = svm.SVC(**tuned_params['SVM'])\n",
    "    svm_mod.fit(xtrain_bs, ytrain_bs)\n",
    "    models['SVM'].update({i: svm_mod})\n",
    "\n",
    "    # random forest\n",
    "    print(f'Training random forest model...')\n",
    "    rf_mod = RandomForestClassifier(**tuned_params['RF'])\n",
    "    rf_mod.fit(xtrain_bs, ytrain_bs)\n",
    "    models['RF'].update({i: rf_mod})\n",
    "    \n",
    "    # xgb\n",
    "    print(f'Training XGBoost model...')\n",
    "    xgb_mod = xgb.XGBClassifier(**tuned_params['XGB'])\n",
    "    xgb_mod.fit(xtrain_bs, ytrain_bs)\n",
    "    models['XGB'].update({i: xgb_mod})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe5997",
   "metadata": {},
   "source": [
    "## Evaluate Model Discrimination on Bootstrapped Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# calculate metrics for each model\n",
    "results_bs = []\n",
    "test_bs = {}\n",
    "for i in range(100):\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "        \n",
    "    # sample from test data\n",
    "    xtest_bs, ytest_bs = bs_samples(xtest, ytest)\n",
    "    test_bs.update({i: [xtest_bs, ytest_bs]})\n",
    "    \n",
    "    ### evaluate model discrimination\n",
    "    for method in models.keys():\n",
    "        metrics_bs = get_metrics(method = method, mod = models[method][i],\n",
    "                                 X_test = xtest_bs, y_test = ytest_bs)\n",
    "        results_bs.append(metrics_bs)\n",
    "\n",
    "results_bs = pd.DataFrame(results_bs, columns = ['Method', 'AUROC', 'Precision', \n",
    "                                                 'Recall', 'F1', 'F2', 'Accuracy'])\n",
    "results_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b94db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate metrics across all boostrapped samples\n",
    "results_CIs = results_bs.groupby(['Method']).quantile([0.025, 0.5, 0.975])\n",
    "results_CIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1cc1e",
   "metadata": {},
   "source": [
    "## Save Median Bootstrapped Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ca713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "### make i=model with median discrimination\n",
    "for key in models.keys():\n",
    "    xtest_bs, ytest_bs = test_bs[i]\n",
    "    median_model = {'Model': models[key][i],\n",
    "                    'xtest': xtest_bs,\n",
    "                    'ytest': ytest_bs}\n",
    "    ## save model\n",
    "    dump(median_model, f'{my_directory}/median_{key}_{batch_date}.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a57ec60",
   "metadata": {},
   "source": [
    "## Evaluate Model Calibration on Bootstrapped Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e098c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# calibrate models and generate calibrated probabilities\n",
    "calib_y_preds = pd.DataFrame()\n",
    "brier_score = []\n",
    "for i in range(100):\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    \n",
    "    # load test data\n",
    "    xtest_bs, ytest_bs = test_bs[i]\n",
    "    \n",
    "    # sample from calibration data\n",
    "    xcalib_bs, ycalib_bs = bs_samples(xcalib, ycalib)\n",
    "\n",
    "    ### calibrate models\n",
    "    for method in models.keys():\n",
    "        mod_calib = CalibratedClassifierCV(models[method][i], cv='prefit')\n",
    "        mod_calib.fit(xcalib_bs, ycalib_bs)\n",
    "        \n",
    "        # generate calibrated predictions\n",
    "        calib_preds = mod_calib.predict_proba(xtest_bs)[:,1]\n",
    "        calib_bs = pd.DataFrame({'method': 'RF',\n",
    "                                 'i': i,\n",
    "                                 'y': test_bs[0][1].values, \n",
    "                                 'pred': calib_preds})\n",
    "        calib_y_preds = pd.concat([calib_y_preds, calib_bs], ignore_index = True)\n",
    "        \n",
    "        # calculate brier score\n",
    "        brier_score_bs = brier_score_loss(ytest_bs, calib_preds)\n",
    "        brier_score.append([method, brier_score_bs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(brier_score, columns = ['Method', 'Brier_Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbff738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### export calibration probabilities to R for Spiegelhalter z-test\n",
    "calib_y_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeceed5",
   "metadata": {},
   "source": [
    "## Top SHAP Features of Median Bootstrapped Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5afe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load median models\n",
    "from joblib import load, dump\n",
    "import shap\n",
    "\n",
    "rf_model = load(f'{my_directory}/median_rf_{batch_date}.joblib')\n",
    "xgb_model = load(f'{my_directory}/median_xgb_{batch_date}.joblib')\n",
    "\n",
    "print('Models loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# SHAP values for median RF\n",
    "print('Generating SHAP explainer...')\n",
    "rf_explainer = shap.Explainer(rf_model['Model'].predict,\n",
    "                              rf_model['xtest'], max_evals=7000) \n",
    "\n",
    "shap_values_rf = rf_explainer(rf_model['xtest'], max_evals=7000)\n",
    "\n",
    "# save SHAP values\n",
    "shap_rf_filename = f'{my_directory}/explainer_rf_{batch_date}.bz2'\n",
    "dump(shap_values_rf, filename = shap_rf_filename, compress=('bz2', 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f762ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# load shap values\n",
    "try:\n",
    "    type(shap_values_rf)\n",
    "except:\n",
    "    print('Loading SHAP explainer...')\n",
    "    shap_rf_filename = f'{my_directory}/explainer_rf_{batch_date}.bz2'\n",
    "    shap_values_rf = load(filename = shap_rf_filename)\n",
    "\n",
    "print('Generating SHAP plot...')\n",
    "shap.plots.beeswarm(shap_values_rf, max_display=20, show = False)\n",
    "plt.savefig(f'{my_directory}/rf_shap_{batch_date}.svg', \n",
    "            format='svg', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add90b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# SHAP values for median XGB\n",
    "print('Generating SHAP explainer...')\n",
    "xgb_explainer = shap.TreeExplainer(xgb_model['Model'])\n",
    "shap_values_xgb = xgb_explainer(xgb_model['xtest'])\n",
    "\n",
    "# plot SHAP values for top 20 features\n",
    "print('Generating SHAP plot...')\n",
    "shap.plots.beeswarm(shap_values_xgb, max_display = 20, show = False)\n",
    "plt.savefig(f'{my_directory}/xgb_shap_{batch_date}.svg', \n",
    "            format='svg', dpi=1200, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
