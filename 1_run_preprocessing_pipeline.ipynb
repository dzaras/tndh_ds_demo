{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a23a08",
   "metadata": {},
   "source": [
    "## Autopsy Reports to Text\n",
    "\n",
    "#### Goal:\n",
    "To extract text from autopsy PDFs\n",
    "\n",
    "#### Input(s):\n",
    "\n",
    "Autopsy PDFs (set path variable specified below)\n",
    "#### Output(s):\n",
    "\n",
    "Feather file with original autopsy text ({my_directory}/new_autopsies_{batch_date}.feather)\n",
    "CSV file with preprocessed narrative text (processed_narr_{batch_date}.csv)\n",
    "#### To run, set 3 variables:\n",
    "\n",
    "my_directory = where you want outputs to save (e.g., 'C:/Users/dc20b49/Documents/TDH_DS_Demo/')\n",
    "path = folder with autopsies (e.g., 'A:/APHA Data Science Grant/2021')\n",
    "batch_date = date you want to appear in output file name (e..g, '8-4-22')\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5081b100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_directory = C:/Users/DC20B46/Desktop/tndh_ds_demo/,\n",
      "path = Z:/APHA Data Science Grant/2021,\n",
      "batch_date = 8-10-22\n"
     ]
    }
   ],
   "source": [
    "%run autopsy_to_text.ipynb\n",
    "\n",
    "my_directory = 'C:/Users/DC20B46/Desktop/tndh_ds_demo/'\n",
    "path = 'Z:/APHA Data Science Grant/2021'\n",
    "batch_date = '8-10-22'\n",
    "\n",
    "print(f'''my_directory = {my_directory},\n",
    "path = {path},\n",
    "batch_date = {batch_date}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040c2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# extract text from each autopsy in directory (this may take hours)...\n",
    "file_paths = list_files(path)\n",
    "print(f'Processing {len(file_paths)} autopsies (this may take hours)...')\n",
    "\n",
    "autopsies = load_autopsies(file_paths)\n",
    "\n",
    "print('Extracting DID, year, first and last names from file names...')\n",
    "\n",
    "autopsies_df = parse_filepath(autopsies)\n",
    "\n",
    "# save original autopsy text to a file\n",
    "autopsies_df.to_feather(f'{my_directory}/new_autopsies_{batch_date}.feather')\n",
    "\n",
    "print(f'Original autopsy text saved to new_autopsies_{batch_date}.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25223282",
   "metadata": {},
   "source": [
    "## Extract Narrative Sections from Autopsy Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99abc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "### run this if you are like me and have autopsies saved in separate files from before; otherwise, ignore\n",
    "# autopsies_sudors = pd.read_feather(f'{my_directory}/autopsies_original_sudors.feather')\n",
    "# autopsies_nonsudors1 = pd.read_feather(f'{my_directory}/autopsies_original_nonOD.feather')\n",
    "# autopsies_nonsudors2 = pd.read_feather(f'{my_directory}/autopsies_original_nonOD_2021.feather')\n",
    "# autopsies_nonsudors = pd.concat([autopsies_nonsudors1, autopsies_nonsudors2], ignore_index=True)\n",
    "# autopsies_nonsudors = autopsies_nonsudors.loc[~autopsies_nonsudors.DID.isin(autopsies_sudors.DID)]\n",
    "# autopsies_df=pd.concat([autopsies_sudors, autopsies_nonsudors], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "# autopsies_df['DID'] = autopsies_df['File_Path'].str.extract('([0-9]+)_')\n",
    "# autopsies_df['full_name'] = autopsies_df['File_Path'].str.lower().str.extract('([a-z\\-\\s\\']+(?:\\.)[a-z\\-\\s\\']+)')\n",
    "# autopsies_df['first_name'] = autopsies_df.apply(lambda x: re.sub('\\..*$', '', x['full_name']) if pd.notna(x['DID'])\n",
    "#                           else re.sub('^.*\\.', '', x['full_name']), axis = 1)\n",
    "# autopsies_df['last_name'] = autopsies_df.apply(lambda x: re.sub('\\..*$', '', x['full_name']) if pd.isna(x['DID'])\n",
    "#                           else re.sub('^.*\\.', '', x['full_name']), axis = 1)\n",
    "# autopsies_df['year'] = autopsies_df.DID.str.extract(r'^([0-9]{4})')\n",
    "# autopsies_df.drop(columns='full_name', inplace = True)\n",
    "# autopsies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "%run extract_narratives.ipynb\n",
    "\n",
    "# remove autopsies from 2010 or with NA years\n",
    "if autopsies_df.loc[(autopsies_df.year == '2010') | (autopsies_df.year.isna())].shape[0] > 0:\n",
    "    print(f'''Autopsies removed because could not extract year from file path, or year appears off:\n",
    "    {autopsies_df.loc[(autopsies_df.year == '2010') | (autopsies_df.year.isna())].File_Path.unique()}\n",
    "    ''')\n",
    "    \n",
    "autopsies_df = autopsies_df.loc[(autopsies_df.year != '2010') & (autopsies_df.year.notna())]\n",
    "\n",
    "# remove whitespaces and symbols\n",
    "print('Cleaning autopsy text...')\n",
    "\n",
    "autopsies_df['doc_clean'] = autopsies_df['doc'].apply(rm_whitespace_sym)\n",
    "\n",
    "# assign forensic centers using regex\n",
    "print('Assigning forensic centers to autopsies...')\n",
    "\n",
    "autopsies_df['forensic_center'] = autopsies_df['doc_clean'].apply(get_forensic_center)\n",
    "autopsies_df['forensic_center'].fillna('', inplace = True)\n",
    "\n",
    "# remove blank autopsies\n",
    "autopsies_df = autopsies_df.loc[autopsies_df.doc_clean != '']\n",
    "\n",
    "# extract narrative sections (initial narrative, interpetation/summary, summary of circumstances)\n",
    "print('Extracting narrative sections from text...')\n",
    "narr_df = get_narrative(autopsies_df)\n",
    "\n",
    "# if more than one narrative for a specific DID, keep the longer one\n",
    "print('Removing duplicate DIDs...')\n",
    "narr_df = narr_df.sort_values(by=['DID', 'full_narr_len'], ascending = False).drop_duplicates(subset='DID').reset_index()\n",
    "\n",
    "# save output\n",
    "narr_df.to_feather(f'{my_directory}/new_narratives_{batch_date}.feather')\n",
    "\n",
    "print(f'Narrative text saved to new_narratives_{batch_date}.feather')\n",
    "\n",
    "# print summary by forensic center\n",
    "narr_df.groupby('forensic_center').agg({'DID': 'nunique',\n",
    "                                        'has_narr': 'sum',\n",
    "                                        'has_interp': 'sum',\n",
    "                                        'has_circ': 'sum',\n",
    "                                        'full_narr_len': ['min', 'max', 'mean']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da70a74e",
   "metadata": {},
   "source": [
    "## Preprocess Narrative Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8fb643",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "%run preprocess_narratives.ipynb\n",
    "\n",
    "narr_df = narr_df[['DID', 'forensic_center', 'year',\n",
    "                   'full_narr', 'has_narr', 'has_interp', 'has_circ']]\n",
    "\n",
    "narr_df['year'] = narr_df['year'].astype(int)\n",
    "narr_df['full_narr_len'] = narr_df['full_narr'].map(len)\n",
    "\n",
    "# preprocess text\n",
    "print('Parsing narrative text (this may take minutes)...')\n",
    "narr_df['full_narr_nlp'] = narr_df['full_narr'].map(nlp)\n",
    "\n",
    "# remove numbers, punctuation, stopwords, and lemmatize\n",
    "print('Removing numbers, punctuation, and stopwords and lemmatizing text...')\n",
    "narr_df['full_narr_lemma'] = (narr_df['full_narr_nlp']\n",
    "                              .map(rm_numbers)\n",
    "                              .map(rm_punct)\n",
    "                              .map(rm_stopwords)\n",
    "                              .map(lemmatize)\n",
    "                             )\n",
    "narr_df['full_narr_lemma_text'] = narr_df['full_narr_lemma'].apply(lambda x: ' '.join(t for t in x))\n",
    "\n",
    "# drop autopsies with no words\n",
    "narr_df = narr_df.loc[narr_df.full_narr_lemma_text.notna()]\n",
    "narr_df['full_narr_lemma_text_len'] = narr_df['full_narr_lemma_text'].map(len)\n",
    "\n",
    "# save output\n",
    "narr_df[['DID', 'forensic_center', 'year', \n",
    "          'has_narr', 'has_interp', 'has_circ',\n",
    "         'full_narr', 'full_narr_lemma',\n",
    "         'full_narr_lemma_text', 'full_narr_lemma_text_len']].to_csv(f'{my_directory}/processed_narr_{batch_date}.csv',\n",
    "                                                                     index = False)\n",
    "\n",
    "print(f'Preprocessed narrative text saved to processed_narr_{batch_date}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
